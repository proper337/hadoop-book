mvn clean install
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Hadoop: The Definitive Guide, Project
[INFO] Common Code
[INFO] Chapter 2: MapReduce
[INFO] Chapter 3: The Hadoop Distributed Filesystem
[INFO] Chapter 4: Hadoop I/O
[INFO] Chapter 5: Developing a MapReduce Application
[INFO] Chapter 7: MapReduce Types and Formats
[INFO] Chapter 8: MapReduce Features
[INFO] Chapter 11: Pig
[INFO] Chapter 12: Hive
[INFO] Chapter 13: HBase
[INFO] Chapter 14: ZooKeeper
[INFO] Chapter 15: Sqoop
[INFO] Chapter 16: Case Studies
[INFO] Hadoop Examples JAR
[INFO] Snippet testing
[INFO] Hadoop: The Definitive Guide, Example Code
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hadoop: The Definitive Guide, Project 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ book ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/book/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ book ---
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ book ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/book/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ book ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ book ---
[INFO] Installing /home/pkeni/git/hadoop-book/book/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/book/3.0/book-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Common Code 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ common ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/common/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ common ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ common ---
[INFO] Compiling 10 source files to /home/pkeni/git/hadoop-book/common/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/common/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ common ---
[INFO] Compiling 3 source files to /home/pkeni/git/hadoop-book/common/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ common ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/common/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running MetOfficeRecordParserTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.093 sec
Running NcdcStationMetadataParserTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 sec
Running NcdcRecordParserTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.019 sec

Results :

Tests run: 10, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ common ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/common/target/common-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ common ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/common/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ common ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/common/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ common ---
[INFO] Installing /home/pkeni/git/hadoop-book/common/target/common-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/common/3.0/common-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/common/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/common/3.0/common-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 2: MapReduce 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch02 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch02/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch02 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch02 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch02/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch02 ---
[INFO] Compiling 9 source files to /home/pkeni/git/hadoop-book/ch02/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch02 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch02/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch02 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch02 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch02 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch02/target/ch02-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch02 ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch02/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch02 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch02 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch02/target/ch02-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch02/3.0/ch02-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch02/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch02/3.0/ch02-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 3: The Hadoop Distributed Filesystem 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch03 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch03/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch03 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch03 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch03/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch03 ---
[INFO] Compiling 8 source files to /home/pkeni/git/hadoop-book/ch03/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch03 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch03/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch03 ---
[INFO] Compiling 4 source files to /home/pkeni/git/hadoop-book/ch03/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch03 ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/ch03/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running CoherencyModelTest
14/01/02 09:09:03 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:03 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:03 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:03 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:03 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:03 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:03 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:03 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:03 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:03 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:03 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:03 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:03 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:04 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:04 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:04 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:04 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:04 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:04 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:04 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:04 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:04 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:04 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:04 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:04 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:04 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:04 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:04 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:04 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:04 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:04 INFO common.Storage: Number of files = 1
14/01/02 09:09:04 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:04 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:04 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:04 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:04 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:04 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:04 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:04 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:04 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:05 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:05 INFO namenode.FSNamesystem: Finished loading FSImage in 561 msecs
14/01/02 09:09:05 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:05 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:05 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:05 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:05 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:05 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:05 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:05 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 7 msec
14/01/02 09:09:05 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs.
14/01/02 09:09:05 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:05 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:05 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 1 msec
14/01/02 09:09:05 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:05 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 1 msec processing time, 1 msec clock time, 1 cycles
14/01/02 09:09:05 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 1 msec
14/01/02 09:09:05 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 1 msec processing time, 1 msec clock time, 1 cycles
14/01/02 09:09:05 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:05 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:37507
14/01/02 09:09:05 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
14/01/02 09:09:05 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:05 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:05 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:05 INFO http.HttpServer: listener.getLocalPort() returned 53970 webServer.getConnectors()[0].getLocalPort() returned 53970
14/01/02 09:09:05 INFO http.HttpServer: Jetty bound to port 53970
14/01/02 09:09:05 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:05 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_53970_hdfs____.v0jv8i/webapp
14/01/02 09:09:05 INFO mortbay.log: Started SelectChannelConnector@localhost:53970
14/01/02 09:09:05 INFO namenode.NameNode: Web-server up at: localhost:53970
14/01/02 09:09:05 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server listener on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 0 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 3 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 2 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 1 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 5 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 4 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 7 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 9 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 8 on 37507: starting
14/01/02 09:09:05 INFO ipc.Server: IPC Server handler 6 on 37507: starting
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
14/01/02 09:09:05 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:05 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at CoherencyModelTest.setUp(CoherencyModelTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:05 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:05 INFO common.Storage: Formatting ...
14/01/02 09:09:05 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:05 INFO common.Storage: Formatting ...
14/01/02 09:09:06 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:06 INFO datanode.DataNode: Opened data transfer server at 50981
14/01/02 09:09:06 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:06 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:06 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:06 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:06 INFO http.HttpServer: listener.getLocalPort() returned 59541 webServer.getConnectors()[0].getLocalPort() returned 59541
14/01/02 09:09:06 INFO http.HttpServer: Jetty bound to port 59541
14/01/02 09:09:06 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:06 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_59541_datanode____.p4vgln/webapp
14/01/02 09:09:06 INFO mortbay.log: Started SelectChannelConnector@localhost:59541
14/01/02 09:09:06 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:50981, storageID=, infoPort=59541, ipcPort=58489)
14/01/02 09:09:06 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:06 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:50981 storage DS-199325623-127.0.1.1-50981-1388682546330
14/01/02 09:09:06 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50981
14/01/02 09:09:06 INFO datanode.DataNode: New storage id DS-199325623-127.0.1.1-50981-1388682546330 is assigned to data-node 127.0.0.1:50981
14/01/02 09:09:06 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:06 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:50981 0 blocks
14/01/02 09:09:06 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:06 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:50981, storageID=DS-199325623-127.0.1.1-50981-1388682546330, infoPort=59541, ipcPort=58489)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:06 INFO datanode.DataNode: Finished asynchronous block report scan in 0ms
14/01/02 09:09:06 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:06 INFO ipc.Server: IPC Server listener on 58489: starting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 1 on 58489: starting
14/01/02 09:09:06 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 2 on 58489: starting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 0 on 58489: starting
14/01/02 09:09:06 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:06 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:50981, blocks: 0, processing time: 1 msecs
14/01/02 09:09:06 INFO datanode.DataNode: BlockReport of 0 blocks took 0 msec to generate and 6 msecs for RPC and NN processing
14/01/02 09:09:06 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:06 INFO datanode.DataNode: Generated rough (lockless) block report in 0 ms
14/01/02 09:09:06 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
Cluster is active
14/01/02 09:09:06 ERROR security.UserGroupInformation: PriviledgedActionException as:pkeni cause:org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /user/pkeni/p File does not exist. Holder DFSClient_NONMAPREDUCE_-1330636610_1 does not have any open files.
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 1 on 37507, call complete(/user/pkeni/p, DFSClient_NONMAPREDUCE_-1330636610_1) from 127.0.0.1:56069: error: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /user/pkeni/p File does not exist. Holder DFSClient_NONMAPREDUCE_-1330636610_1 does not have any open files.
org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /user/pkeni/p File does not exist. Holder DFSClient_NONMAPREDUCE_-1330636610_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1720)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1711)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1766)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1754)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:751)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)
14/01/02 09:09:06 ERROR hdfs.DFSClient: Failed to close file /user/pkeni/p
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /user/pkeni/p File does not exist. Holder DFSClient_NONMAPREDUCE_-1330636610_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1720)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1711)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1766)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1754)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:751)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)
	at java.security.AccessController.doPrivileged(Native Method)
Shutting down the Mini HDFS Cluster
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)

	at org.apache.hadoop.ipc.Client.call(Client.java:1107)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
Shutting down DataNode 0
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:4087)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3988)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:413)
	at org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:429)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:339)
	at CoherencyModelTest.tearDown(CoherencyModelTest.java:37)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:06 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:06 INFO ipc.Server: Stopping server on 58489
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 0 on 58489: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 1 on 58489: exiting
14/01/02 09:09:06 INFO ipc.Server: Stopping IPC Server listener on 58489
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 2 on 58489: exiting
14/01/02 09:09:06 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:06 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:06 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:50981, storageID=DS-199325623-127.0.1.1-50981-1388682546330, infoPort=59541, ipcPort=58489):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:09:06 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:09:06 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 1
14/01/02 09:09:06 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:09:06 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:50981, storageID=DS-199325623-127.0.1.1-50981-1388682546330, infoPort=59541, ipcPort=58489):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:06 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:06 INFO ipc.Server: Stopping server on 58489
14/01/02 09:09:06 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:06 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:06 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:09:06 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:09:06 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-908646569
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-908646569
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at CoherencyModelTest.tearDown(CoherencyModelTest.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:06 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
14/01/02 09:09:06 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:06 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:09:06 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:06 INFO namenode.FSNamesystem: Number of transactions: 5 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 142 62 
14/01/02 09:09:06 INFO namenode.FSEditLog: closing edit log: position=287, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:06 INFO namenode.FSEditLog: close success: truncate to 287, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:06 INFO namenode.FSEditLog: closing edit log: position=287, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:06 INFO namenode.FSEditLog: close success: truncate to 287, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:06 INFO ipc.Server: Stopping server on 37507
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 0 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 1 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 3 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 2 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 6 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 5 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 7 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 9 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 4 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: IPC Server handler 8 on 37507: exiting
14/01/02 09:09:06 INFO ipc.Server: Stopping IPC Server listener on 37507
14/01/02 09:09:06 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:06 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:06 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:06 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:06 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:06 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:06 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:06 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:06 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:06 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:06 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:06 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:06 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:07 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:07 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:07 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:07 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:07 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:07 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:07 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:07 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:07 WARN util.MBeans: Hadoop:service=NameNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1402)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at CoherencyModelTest.setUp(CoherencyModelTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:07 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:07 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:07 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:07 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:07 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:07 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:07 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:07 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:07 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:07 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:07 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:07 INFO common.Storage: Number of files = 1
14/01/02 09:09:07 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:07 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:07 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:07 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:07 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:07 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:07 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:07 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:07 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:08 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:08 INFO namenode.FSNamesystem: Finished loading FSImage in 557 msecs
14/01/02 09:09:08 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:08 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:08 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:08 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:08 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:08 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:08 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:08 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 3 msec
14/01/02 09:09:08 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs.
14/01/02 09:09:08 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:08 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:08 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:08 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:08 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:08 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:37997
14/01/02 09:09:08 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:08 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:08 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:08 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:08 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:08 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:08 INFO http.HttpServer: listener.getLocalPort() returned 56941 webServer.getConnectors()[0].getLocalPort() returned 56941
14/01/02 09:09:08 INFO http.HttpServer: Jetty bound to port 56941
14/01/02 09:09:08 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:08 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_56941_hdfs____qa04wv/webapp
14/01/02 09:09:08 INFO mortbay.log: Started SelectChannelConnector@localhost:56941
14/01/02 09:09:08 INFO namenode.NameNode: Web-server up at: localhost:56941
14/01/02 09:09:08 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 0 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server listener on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 1 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 2 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 4 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 5 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 3 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 6 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 9 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 8 on 37997: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 7 on 37997: starting
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
14/01/02 09:09:08 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:08 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at CoherencyModelTest.setUp(CoherencyModelTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:08 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:08 INFO common.Storage: Formatting ...
14/01/02 09:09:08 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:08 INFO common.Storage: Formatting ...
14/01/02 09:09:08 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:08 INFO datanode.DataNode: Opened data transfer server at 40162
14/01/02 09:09:08 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:08 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:08 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:08 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:08 INFO http.HttpServer: listener.getLocalPort() returned 60867 webServer.getConnectors()[0].getLocalPort() returned 60867
14/01/02 09:09:08 INFO http.HttpServer: Jetty bound to port 60867
14/01/02 09:09:08 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:08 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_60867_datanode____v5xv1i/webapp
14/01/02 09:09:08 INFO mortbay.log: Started SelectChannelConnector@localhost:60867
14/01/02 09:09:08 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:08 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:40162, storageID=, infoPort=60867, ipcPort=54049)
14/01/02 09:09:08 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:40162 storage DS-1228929658-127.0.1.1-40162-1388682548942
14/01/02 09:09:08 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:40162
14/01/02 09:09:08 INFO datanode.DataNode: New storage id DS-1228929658-127.0.1.1-40162-1388682548942 is assigned to data-node 127.0.0.1:40162
14/01/02 09:09:08 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:08 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:40162 0 blocks
14/01/02 09:09:08 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:08 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:40162, storageID=DS-1228929658-127.0.1.1-40162-1388682548942, infoPort=60867, ipcPort=54049)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:08 INFO datanode.DataNode: Finished asynchronous block report scan in 1ms
14/01/02 09:09:08 INFO ipc.Server: IPC Server listener on 54049: starting
14/01/02 09:09:08 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 0 on 54049: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 2 on 54049: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:08 INFO ipc.Server: IPC Server handler 1 on 54049: starting
14/01/02 09:09:08 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:08 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:40162, blocks: 0, processing time: 0 msecs
14/01/02 09:09:08 INFO datanode.DataNode: BlockReport of 0 blocks took 0 msec to generate and 1 msecs for RPC and NN processing
14/01/02 09:09:08 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:08 INFO datanode.DataNode: Generated rough (lockless) block report in 1 ms
14/01/02 09:09:08 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
Cluster is active
14/01/02 09:09:09 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/pkeni/p. blk_1762853788756619511_1001
14/01/02 09:09:09 INFO datanode.DataNode: Receiving block blk_1762853788756619511_1001 src: /127.0.0.1:53505 dest: /127.0.0.1:40162
14/01/02 09:09:09 INFO DataNode.clienttrace: src: /127.0.0.1:53505, dest: /127.0.0.1:40162, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-53828852_1, offset: 0, srvID: DS-1228929658-127.0.1.1-40162-1388682548942, blockid: blk_1762853788756619511_1001, duration: 217474
14/01/02 09:09:09 INFO datanode.DataNode: PacketResponder 0 for block blk_1762853788756619511_1001 terminating
14/01/02 09:09:09 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:40162 is added to blk_1762853788756619511_1001 size 7
14/01/02 09:09:09 INFO hdfs.StateChange: Removing lease on  file /user/pkeni/p from client DFSClient_NONMAPREDUCE_-53828852_1
14/01/02 09:09:09 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /user/pkeni/p is closed by DFSClient_NONMAPREDUCE_-53828852_1
14/01/02 09:09:09 INFO hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_1762853788756619511 to 127.0.0.1:40162 
Shutting down the Mini HDFS Cluster
Shutting down DataNode 0
14/01/02 09:09:09 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:09 INFO ipc.Server: Stopping server on 54049
14/01/02 09:09:09 INFO ipc.Server: IPC Server handler 0 on 54049: exiting
14/01/02 09:09:09 INFO ipc.Server: Stopping IPC Server listener on 54049
14/01/02 09:09:09 INFO ipc.Server: IPC Server handler 1 on 54049: exiting
14/01/02 09:09:09 INFO ipc.Server: IPC Server handler 2 on 54049: exiting
14/01/02 09:09:09 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:09 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:09 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 1
14/01/02 09:09:09 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:40162, storageID=DS-1228929658-127.0.1.1-40162-1388682548942, infoPort=60867, ipcPort=54049):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:09:09 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:09:09 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:09:10 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:10 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:40162, storageID=DS-1228929658-127.0.1.1-40162-1388682548942, infoPort=60867, ipcPort=54049):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:10 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:10 INFO ipc.Server: Stopping server on 54049
14/01/02 09:09:10 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:10 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:10 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:09:10 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:09:10 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId787688845
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId787688845
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at CoherencyModelTest.tearDown(CoherencyModelTest.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:10 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
14/01/02 09:09:10 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:10 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:09:10 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:10 INFO namenode.FSNamesystem: Number of transactions: 7 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 108 82 
14/01/02 09:09:10 INFO namenode.FSEditLog: closing edit log: position=552, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:10 INFO namenode.FSEditLog: close success: truncate to 552, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:10 INFO namenode.FSEditLog: closing edit log: position=552, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:10 INFO namenode.FSEditLog: close success: truncate to 552, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:10 INFO ipc.Server: Stopping server on 37997
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 0 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 1 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 3 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 2 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 4 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 5 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 6 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 9 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 8 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: IPC Server handler 7 on 37997: exiting
14/01/02 09:09:10 INFO ipc.Server: Stopping IPC Server listener on 37997
14/01/02 09:09:10 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:10 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:10 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:10 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:10 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:10 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:10 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:10 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:10 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:10 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:10 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:10 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:10 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:10 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:10 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:10 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:10 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:11 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:11 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:11 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:11 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:11 WARN util.MBeans: Hadoop:service=NameNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1402)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at CoherencyModelTest.setUp(CoherencyModelTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:11 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:11 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:11 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:11 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:11 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:11 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:11 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:11 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:11 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:11 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:11 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:11 INFO common.Storage: Number of files = 1
14/01/02 09:09:11 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:11 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:11 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:11 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:11 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:11 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:11 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:11 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:11 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:11 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:11 INFO namenode.FSNamesystem: Finished loading FSImage in 590 msecs
14/01/02 09:09:11 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:11 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:11 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:11 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:11 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:11 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:11 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:11 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 1 msec
14/01/02 09:09:11 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs.
14/01/02 09:09:11 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:11 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:11 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:11 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:11 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:35026
14/01/02 09:09:11 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:11 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:11 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:11 INFO http.HttpServer: listener.getLocalPort() returned 35974 webServer.getConnectors()[0].getLocalPort() returned 35974
14/01/02 09:09:11 INFO http.HttpServer: Jetty bound to port 35974
14/01/02 09:09:11 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:11 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 3 msec
14/01/02 09:09:11 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 3 msec processing time, 3 msec clock time, 1 cycles
14/01/02 09:09:11 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:11 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:11 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_35974_hdfs____5gatpa/webapp
14/01/02 09:09:11 INFO mortbay.log: Started SelectChannelConnector@localhost:35974
14/01/02 09:09:11 INFO namenode.NameNode: Web-server up at: localhost:35974
14/01/02 09:09:11 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server listener on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 1 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 0 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 2 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 3 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 4 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 5 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 9 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 6 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 8 on 35026: starting
14/01/02 09:09:11 INFO ipc.Server: IPC Server handler 7 on 35026: starting
14/01/02 09:09:11 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:11 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at CoherencyModelTest.setUp(CoherencyModelTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:11 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:11 INFO common.Storage: Formatting ...
14/01/02 09:09:12 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:12 INFO common.Storage: Formatting ...
14/01/02 09:09:12 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:12 INFO datanode.DataNode: Opened data transfer server at 43143
14/01/02 09:09:12 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:12 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:12 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:12 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:12 INFO http.HttpServer: listener.getLocalPort() returned 55234 webServer.getConnectors()[0].getLocalPort() returned 55234
14/01/02 09:09:12 INFO http.HttpServer: Jetty bound to port 55234
14/01/02 09:09:12 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:12 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_55234_datanode____.1bnl7q/webapp
14/01/02 09:09:12 INFO mortbay.log: Started SelectChannelConnector@localhost:55234
14/01/02 09:09:12 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:12 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:43143, storageID=, infoPort=55234, ipcPort=59742)
14/01/02 09:09:12 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:43143 storage DS-682035950-127.0.1.1-43143-1388682552702
14/01/02 09:09:12 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:43143
14/01/02 09:09:12 INFO datanode.DataNode: New storage id DS-682035950-127.0.1.1-43143-1388682552702 is assigned to data-node 127.0.0.1:43143
14/01/02 09:09:12 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:12 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:43143 0 blocks
14/01/02 09:09:12 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:12 INFO datanode.DataNode: Finished asynchronous block report scan in 0ms
14/01/02 09:09:12 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:43143, storageID=DS-682035950-127.0.1.1-43143-1388682552702, infoPort=55234, ipcPort=59742)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:12 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:12 INFO ipc.Server: IPC Server handler 1 on 59742: starting
14/01/02 09:09:12 INFO ipc.Server: IPC Server handler 0 on 59742: starting
14/01/02 09:09:12 INFO ipc.Server: IPC Server listener on 59742: starting
14/01/02 09:09:12 INFO ipc.Server: IPC Server handler 2 on 59742: starting
14/01/02 09:09:12 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:12 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:12 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:43143, blocks: 0, processing time: 0 msecs
14/01/02 09:09:12 INFO datanode.DataNode: BlockReport of 0 blocks took 0 msec to generate and 2 msecs for RPC and NN processing
14/01/02 09:09:12 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:12 INFO datanode.DataNode: Generated rough (lockless) block report in 0 ms
14/01/02 09:09:12 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
Cluster is active
Shutting down the Mini HDFS Cluster
Shutting down DataNode 0
14/01/02 09:09:12 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:13 INFO ipc.Server: Stopping server on 59742
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 0 on 59742: exiting
14/01/02 09:09:13 INFO ipc.Server: Stopping IPC Server listener on 59742
14/01/02 09:09:13 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:13 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:13 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:43143, storageID=DS-682035950-127.0.1.1-43143-1388682552702, infoPort=55234, ipcPort=59742):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:09:13 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 1 on 59742: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 2 on 59742: exiting
14/01/02 09:09:13 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:13 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:09:13 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:43143, storageID=DS-682035950-127.0.1.1-43143-1388682552702, infoPort=55234, ipcPort=59742):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:13 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:13 INFO ipc.Server: Stopping server on 59742
14/01/02 09:09:13 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:13 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:13 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:09:13 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:09:13 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId1785030348
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId1785030348
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at CoherencyModelTest.tearDown(CoherencyModelTest.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:13 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
14/01/02 09:09:13 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:13 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:09:13 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:13 INFO namenode.FSNamesystem: Number of transactions: 0 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 0 
14/01/02 09:09:13 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:13 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:13 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:13 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:13 INFO ipc.Server: Stopping server on 35026
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 1 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 0 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 2 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 3 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 5 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 7 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 6 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 9 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 8 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: IPC Server handler 4 on 35026: exiting
14/01/02 09:09:13 INFO ipc.Server: Stopping IPC Server listener on 35026
14/01/02 09:09:13 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:13 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:13 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:13 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:13 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:13 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:13 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:13 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:13 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:13 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:13 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:13 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:13 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:13 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:13 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:13 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:13 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:13 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:13 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:13 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:13 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:13 WARN util.MBeans: Hadoop:service=NameNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1402)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at CoherencyModelTest.setUp(CoherencyModelTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:13 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:13 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:13 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:13 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:13 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:13 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:13 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:13 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:13 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:13 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:13 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:13 INFO common.Storage: Number of files = 1
14/01/02 09:09:13 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:13 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:13 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:13 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:13 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:13 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:14 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:14 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:14 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:14 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:14 INFO namenode.FSNamesystem: Finished loading FSImage in 884 msecs
14/01/02 09:09:14 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:14 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:14 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:14 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:14 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:14 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:14 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:14 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 2 msec
14/01/02 09:09:14 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs.
14/01/02 09:09:14 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:14 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:14 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:14 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:14 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 1 msec
14/01/02 09:09:14 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 1 msec processing time, 1 msec clock time, 1 cycles
14/01/02 09:09:14 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:14 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:46291
14/01/02 09:09:14 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:14 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:14 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:14 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:14 INFO http.HttpServer: listener.getLocalPort() returned 42208 webServer.getConnectors()[0].getLocalPort() returned 42208
14/01/02 09:09:14 INFO http.HttpServer: Jetty bound to port 42208
14/01/02 09:09:14 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:14 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_42208_hdfs____ydi23q/webapp
14/01/02 09:09:14 INFO mortbay.log: Started SelectChannelConnector@localhost:42208
14/01/02 09:09:14 INFO namenode.NameNode: Web-server up at: localhost:42208
14/01/02 09:09:14 INFO ipc.Server: IPC Server listener on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 0 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 2 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 4 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 8 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 3 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 1 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 9 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 7 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 6 on 46291: starting
14/01/02 09:09:14 INFO ipc.Server: IPC Server handler 5 on 46291: starting
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
14/01/02 09:09:14 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:14 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at CoherencyModelTest.setUp(CoherencyModelTest.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:14 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:14 INFO common.Storage: Formatting ...
14/01/02 09:09:15 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:15 INFO common.Storage: Formatting ...
14/01/02 09:09:15 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:15 INFO datanode.DataNode: Opened data transfer server at 32822
14/01/02 09:09:15 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:15 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:15 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:15 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:15 INFO http.HttpServer: listener.getLocalPort() returned 35905 webServer.getConnectors()[0].getLocalPort() returned 35905
14/01/02 09:09:15 INFO http.HttpServer: Jetty bound to port 35905
14/01/02 09:09:15 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:15 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_35905_datanode____6lvi9v/webapp
14/01/02 09:09:15 INFO mortbay.log: Started SelectChannelConnector@localhost:35905
14/01/02 09:09:15 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:15 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:32822, storageID=, infoPort=35905, ipcPort=48060)
14/01/02 09:09:15 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:32822 storage DS-1733800996-127.0.1.1-32822-1388682555528
14/01/02 09:09:15 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:32822
14/01/02 09:09:15 INFO datanode.DataNode: New storage id DS-1733800996-127.0.1.1-32822-1388682555528 is assigned to data-node 127.0.0.1:32822
14/01/02 09:09:15 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:15 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:32822 0 blocks
14/01/02 09:09:15 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:15 INFO datanode.DataNode: Finished asynchronous block report scan in 2ms
14/01/02 09:09:15 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:32822, storageID=DS-1733800996-127.0.1.1-32822-1388682555528, infoPort=35905, ipcPort=48060)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:15 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:15 INFO ipc.Server: IPC Server listener on 48060: starting
14/01/02 09:09:15 INFO ipc.Server: IPC Server handler 0 on 48060: starting
14/01/02 09:09:15 INFO ipc.Server: IPC Server handler 1 on 48060: starting
14/01/02 09:09:15 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:15 INFO ipc.Server: IPC Server handler 2 on 48060: starting
Cluster is active
14/01/02 09:09:15 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:15 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:32822, blocks: 0, processing time: 0 msecs
14/01/02 09:09:15 INFO datanode.DataNode: BlockReport of 0 blocks took 3 msec to generate and 2 msecs for RPC and NN processing
14/01/02 09:09:15 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:15 INFO datanode.DataNode: Generated rough (lockless) block report in 1 ms
14/01/02 09:09:15 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:15 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/pkeni/p. blk_689274854729528576_1001
14/01/02 09:09:15 INFO datanode.DataNode: Receiving block blk_689274854729528576_1001 src: /127.0.0.1:59629 dest: /127.0.0.1:32822
14/01/02 09:09:15 INFO DataNode.clienttrace: src: /127.0.0.1:59629, dest: /127.0.0.1:32822, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1592773196_1, offset: 0, srvID: DS-1733800996-127.0.1.1-32822-1388682555528, blockid: blk_689274854729528576_1001, duration: 287723
14/01/02 09:09:15 INFO datanode.DataNode: PacketResponder 0 for block blk_689274854729528576_1001 terminating
14/01/02 09:09:15 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:32822 is added to blk_689274854729528576_1001 size 7
14/01/02 09:09:16 INFO hdfs.StateChange: Removing lease on  file /user/pkeni/p from client DFSClient_NONMAPREDUCE_1592773196_1
14/01/02 09:09:16 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /user/pkeni/p is closed by DFSClient_NONMAPREDUCE_1592773196_1
14/01/02 09:09:16 INFO hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_689274854729528576 to 127.0.0.1:32822 
Shutting down the Mini HDFS Cluster
Shutting down DataNode 0
14/01/02 09:09:16 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:16 INFO ipc.Server: Stopping server on 48060
14/01/02 09:09:16 INFO ipc.Server: IPC Server handler 1 on 48060: exiting
14/01/02 09:09:16 INFO ipc.Server: IPC Server handler 0 on 48060: exiting
14/01/02 09:09:16 INFO ipc.Server: IPC Server handler 2 on 48060: exiting
14/01/02 09:09:16 INFO ipc.Server: Stopping IPC Server listener on 48060
14/01/02 09:09:16 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:16 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:16 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 1
14/01/02 09:09:16 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:32822, storageID=DS-1733800996-127.0.1.1-32822-1388682555528, infoPort=35905, ipcPort=48060):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:09:16 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:09:16 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:09:17 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:17 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:32822, storageID=DS-1733800996-127.0.1.1-32822-1388682555528, infoPort=35905, ipcPort=48060):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:17 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:17 INFO ipc.Server: Stopping server on 48060
14/01/02 09:09:17 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:17 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:17 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:09:17 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:09:17 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1739920885
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1739920885
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at CoherencyModelTest.tearDown(CoherencyModelTest.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:17 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
14/01/02 09:09:17 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:17 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:09:17 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:17 INFO namenode.FSNamesystem: Number of transactions: 7 Total time for transactions(ms): 2Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 145 127 
14/01/02 09:09:17 INFO namenode.FSEditLog: closing edit log: position=554, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:17 INFO namenode.FSEditLog: close success: truncate to 554, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:17 INFO namenode.FSEditLog: closing edit log: position=554, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:17 INFO namenode.FSEditLog: close success: truncate to 554, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:17 INFO ipc.Server: Stopping server on 46291
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 0 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 1 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 2 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 3 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 5 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: Stopping IPC Server listener on 46291
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 6 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 7 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 8 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 9 on 46291: exiting
14/01/02 09:09:17 INFO ipc.Server: IPC Server handler 4 on 46291: exiting
14/01/02 09:09:17 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:17 INFO ipc.Server: Stopping IPC Server Responder
Tests run: 5, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 14.685 sec
Running FileSystemDeleteTest
14/01/02 09:09:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.072 sec
Running ShowFileStatusTest
14/01/02 09:09:17 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:17 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:17 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:17 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:17 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:17 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:17 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:17 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:17 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:17 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:17 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:17 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:17 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:18 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:18 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:18 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:18 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:18 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:18 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:18 WARN util.MBeans: Hadoop:service=NameNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1402)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at ShowFileStatusTest.setUp(ShowFileStatusTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:18 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:18 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:18 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:18 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:18 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:18 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:18 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:18 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:18 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:18 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:18 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:18 INFO common.Storage: Number of files = 1
14/01/02 09:09:18 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:18 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:18 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:18 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:18 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:18 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:18 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:18 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:18 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:18 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:18 INFO namenode.FSNamesystem: Finished loading FSImage in 585 msecs
14/01/02 09:09:18 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:18 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:18 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:18 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:18 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:18 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:18 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:18 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 1 msec
14/01/02 09:09:18 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs.
14/01/02 09:09:18 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:18 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:18 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:18 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 1 msec
14/01/02 09:09:18 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 1 msec processing time, 1 msec clock time, 1 cycles
14/01/02 09:09:18 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:18 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:18 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:18 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:33611
14/01/02 09:09:18 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:18 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:18 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:18 INFO http.HttpServer: listener.getLocalPort() returned 35658 webServer.getConnectors()[0].getLocalPort() returned 35658
14/01/02 09:09:18 INFO http.HttpServer: Jetty bound to port 35658
14/01/02 09:09:18 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:18 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_35658_hdfs____.igkm8v/webapp
14/01/02 09:09:19 INFO mortbay.log: Started SelectChannelConnector@localhost:35658
14/01/02 09:09:19 INFO namenode.NameNode: Web-server up at: localhost:35658
14/01/02 09:09:19 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server listener on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 0 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 1 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 3 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 4 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 2 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 5 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 7 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 6 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 8 on 33611: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 9 on 33611: starting
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
14/01/02 09:09:19 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:19 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at ShowFileStatusTest.setUp(ShowFileStatusTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:19 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:19 INFO common.Storage: Formatting ...
14/01/02 09:09:19 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:19 INFO common.Storage: Formatting ...
14/01/02 09:09:19 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:19 INFO datanode.DataNode: Opened data transfer server at 54752
14/01/02 09:09:19 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:19 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:19 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:19 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:19 INFO http.HttpServer: listener.getLocalPort() returned 48799 webServer.getConnectors()[0].getLocalPort() returned 48799
14/01/02 09:09:19 INFO http.HttpServer: Jetty bound to port 48799
14/01/02 09:09:19 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:19 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_48799_datanode____174l4q/webapp
14/01/02 09:09:19 INFO mortbay.log: Started SelectChannelConnector@localhost:48799
14/01/02 09:09:19 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:19 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:54752, storageID=, infoPort=48799, ipcPort=35595)
14/01/02 09:09:19 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:54752 storage DS-1375062259-127.0.1.1-54752-1388682559677
14/01/02 09:09:19 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:54752
14/01/02 09:09:19 INFO datanode.DataNode: New storage id DS-1375062259-127.0.1.1-54752-1388682559677 is assigned to data-node 127.0.0.1:54752
14/01/02 09:09:19 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:19 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:54752 0 blocks
14/01/02 09:09:19 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:19 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:54752, storageID=DS-1375062259-127.0.1.1-54752-1388682559677, infoPort=48799, ipcPort=35595)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:19 INFO datanode.DataNode: Finished asynchronous block report scan in 1ms
14/01/02 09:09:19 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:19 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 2 on 35595: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 1 on 35595: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server handler 0 on 35595: starting
14/01/02 09:09:19 INFO ipc.Server: IPC Server listener on 35595: starting
14/01/02 09:09:19 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:19 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:54752, blocks: 0, processing time: 0 msecs
Cluster is active
14/01/02 09:09:19 INFO datanode.DataNode: BlockReport of 0 blocks took 1 msec to generate and 0 msecs for RPC and NN processing
14/01/02 09:09:19 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:19 INFO datanode.DataNode: Generated rough (lockless) block report in 0 ms
14/01/02 09:09:19 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:19 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /dir/file. blk_-7998249725510623806_1001
14/01/02 09:09:19 INFO datanode.DataNode: Receiving block blk_-7998249725510623806_1001 src: /127.0.0.1:45459 dest: /127.0.0.1:54752
14/01/02 09:09:19 INFO DataNode.clienttrace: src: /127.0.0.1:45459, dest: /127.0.0.1:54752, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-730372589_1, offset: 0, srvID: DS-1375062259-127.0.1.1-54752-1388682559677, blockid: blk_-7998249725510623806_1001, duration: 303654
14/01/02 09:09:19 INFO datanode.DataNode: PacketResponder 0 for block blk_-7998249725510623806_1001 terminating
14/01/02 09:09:19 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:54752 is added to blk_-7998249725510623806_1001 size 7
14/01/02 09:09:20 INFO hdfs.StateChange: Removing lease on  file /dir/file from client DFSClient_NONMAPREDUCE_-730372589_1
14/01/02 09:09:20 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /dir/file is closed by DFSClient_NONMAPREDUCE_-730372589_1
Shutting down the Mini HDFS Cluster
Shutting down DataNode 0
14/01/02 09:09:20 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:20 INFO ipc.Server: Stopping server on 35595
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 1 on 35595: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 0 on 35595: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 2 on 35595: exiting
14/01/02 09:09:20 INFO ipc.Server: Stopping IPC Server listener on 35595
14/01/02 09:09:20 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:20 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:20 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:54752, storageID=DS-1375062259-127.0.1.1-54752-1388682559677, infoPort=48799, ipcPort=35595):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:09:20 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:09:20 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 1
14/01/02 09:09:20 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:09:20 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:54752, storageID=DS-1375062259-127.0.1.1-54752-1388682559677, infoPort=48799, ipcPort=35595):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:20 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:20 INFO ipc.Server: Stopping server on 35595
14/01/02 09:09:20 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:20 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:20 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:09:20 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:09:20 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1886686865
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1886686865
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at ShowFileStatusTest.tearDown(ShowFileStatusTest.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:20 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
14/01/02 09:09:20 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:20 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:09:20 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:20 INFO namenode.FSNamesystem: Number of transactions: 5 Total time for transactions(ms): 1Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 138 108 
14/01/02 09:09:20 INFO namenode.FSEditLog: closing edit log: position=451, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:20 INFO namenode.FSEditLog: close success: truncate to 451, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:20 INFO namenode.FSEditLog: closing edit log: position=451, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:20 INFO namenode.FSEditLog: close success: truncate to 451, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:20 INFO ipc.Server: Stopping server on 33611
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 0 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 1 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 2 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 3 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 6 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 7 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: Stopping IPC Server listener on 33611
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 4 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 5 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 8 on 33611: exiting
14/01/02 09:09:20 INFO ipc.Server: IPC Server handler 9 on 33611: exiting
14/01/02 09:09:20 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:20 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:20 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:20 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:20 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:20 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:20 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:20 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:20 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:20 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:20 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:20 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:20 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:20 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:20 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:21 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:21 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:21 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:21 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:21 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:21 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:21 WARN util.MBeans: Hadoop:service=NameNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1402)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at ShowFileStatusTest.setUp(ShowFileStatusTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:21 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:21 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:21 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:21 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:21 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:21 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:21 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:21 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:21 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:21 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:21 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:21 INFO common.Storage: Number of files = 1
14/01/02 09:09:21 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:21 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:21 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:21 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:21 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:21 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:21 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:21 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:21 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:22 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:22 INFO namenode.FSNamesystem: Finished loading FSImage in 590 msecs
14/01/02 09:09:22 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:22 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:22 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:22 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:22 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:22 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:22 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:22 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 1 msec
14/01/02 09:09:22 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs.
14/01/02 09:09:22 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:22 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:22 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:22 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:22 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:22 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:22 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:22 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:22 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:53610
14/01/02 09:09:22 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:22 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:22 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:22 INFO http.HttpServer: listener.getLocalPort() returned 48603 webServer.getConnectors()[0].getLocalPort() returned 48603
14/01/02 09:09:22 INFO http.HttpServer: Jetty bound to port 48603
14/01/02 09:09:22 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:22 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_48603_hdfs____6qmn33/webapp
14/01/02 09:09:22 INFO mortbay.log: Started SelectChannelConnector@localhost:48603
14/01/02 09:09:22 INFO namenode.NameNode: Web-server up at: localhost:48603
14/01/02 09:09:22 INFO ipc.Server: IPC Server listener on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 0 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 1 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 3 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 4 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 2 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 9 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 8 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 5 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 6 on 53610: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 7 on 53610: starting
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
14/01/02 09:09:22 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:22 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at ShowFileStatusTest.setUp(ShowFileStatusTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:22 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:22 INFO common.Storage: Formatting ...
14/01/02 09:09:22 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:22 INFO common.Storage: Formatting ...
14/01/02 09:09:22 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:22 INFO datanode.DataNode: Opened data transfer server at 38631
14/01/02 09:09:22 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:22 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:22 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:22 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:22 INFO http.HttpServer: listener.getLocalPort() returned 47052 webServer.getConnectors()[0].getLocalPort() returned 47052
14/01/02 09:09:22 INFO http.HttpServer: Jetty bound to port 47052
14/01/02 09:09:22 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:22 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_47052_datanode____.qfxajz/webapp
14/01/02 09:09:22 INFO mortbay.log: Started SelectChannelConnector@localhost:47052
14/01/02 09:09:22 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:22 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:38631, storageID=, infoPort=47052, ipcPort=53284)
14/01/02 09:09:22 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:38631 storage DS-1646712920-127.0.1.1-38631-1388682562814
14/01/02 09:09:22 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:38631
14/01/02 09:09:22 INFO datanode.DataNode: New storage id DS-1646712920-127.0.1.1-38631-1388682562814 is assigned to data-node 127.0.0.1:38631
14/01/02 09:09:22 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:22 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:38631 0 blocks
14/01/02 09:09:22 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:22 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:38631, storageID=DS-1646712920-127.0.1.1-38631-1388682562814, infoPort=47052, ipcPort=53284)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:22 INFO datanode.DataNode: Finished asynchronous block report scan in 1ms
14/01/02 09:09:22 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server listener on 53284: starting
14/01/02 09:09:22 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 2 on 53284: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 1 on 53284: starting
14/01/02 09:09:22 INFO ipc.Server: IPC Server handler 0 on 53284: starting
14/01/02 09:09:22 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:22 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:38631, blocks: 0, processing time: 0 msecs
Cluster is active
14/01/02 09:09:22 INFO datanode.DataNode: BlockReport of 0 blocks took 0 msec to generate and 2 msecs for RPC and NN processing
14/01/02 09:09:22 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:22 INFO datanode.DataNode: Generated rough (lockless) block report in 0 ms
14/01/02 09:09:22 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:23 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /dir/file. blk_-191540979323105190_1001
14/01/02 09:09:23 INFO datanode.DataNode: Receiving block blk_-191540979323105190_1001 src: /127.0.0.1:36611 dest: /127.0.0.1:38631
14/01/02 09:09:23 INFO DataNode.clienttrace: src: /127.0.0.1:36611, dest: /127.0.0.1:38631, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-331650887_1, offset: 0, srvID: DS-1646712920-127.0.1.1-38631-1388682562814, blockid: blk_-191540979323105190_1001, duration: 442668
14/01/02 09:09:23 INFO datanode.DataNode: PacketResponder 0 for block blk_-191540979323105190_1001 terminating
14/01/02 09:09:23 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:38631 is added to blk_-191540979323105190_1001 size 7
14/01/02 09:09:23 INFO hdfs.StateChange: Removing lease on  file /dir/file from client DFSClient_NONMAPREDUCE_-331650887_1
14/01/02 09:09:23 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /dir/file is closed by DFSClient_NONMAPREDUCE_-331650887_1
Shutting down the Mini HDFS Cluster
Shutting down DataNode 0
14/01/02 09:09:23 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:23 INFO ipc.Server: Stopping server on 53284
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 1 on 53284: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 0 on 53284: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 2 on 53284: exiting
14/01/02 09:09:23 INFO ipc.Server: Stopping IPC Server listener on 53284
14/01/02 09:09:23 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:23 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:23 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:38631, storageID=DS-1646712920-127.0.1.1-38631-1388682562814, infoPort=47052, ipcPort=53284):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:09:23 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:09:23 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 1
14/01/02 09:09:23 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:09:23 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:38631, storageID=DS-1646712920-127.0.1.1-38631-1388682562814, infoPort=47052, ipcPort=53284):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:23 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:23 INFO ipc.Server: Stopping server on 53284
14/01/02 09:09:23 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:23 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:23 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:09:23 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:09:23 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1469823477
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-1469823477
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at ShowFileStatusTest.tearDown(ShowFileStatusTest.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:23 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
14/01/02 09:09:23 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:23 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:09:23 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:23 INFO namenode.FSNamesystem: Number of transactions: 5 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 122 93 
14/01/02 09:09:23 INFO namenode.FSEditLog: closing edit log: position=451, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:23 INFO namenode.FSEditLog: close success: truncate to 451, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:23 INFO namenode.FSEditLog: closing edit log: position=451, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:23 INFO namenode.FSEditLog: close success: truncate to 451, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:23 INFO ipc.Server: Stopping server on 53610
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 0 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 1 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 3 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 4 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 6 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 8 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 9 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 2 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 7 on 53610: exiting
14/01/02 09:09:23 INFO ipc.Server: Stopping IPC Server listener on 53610
14/01/02 09:09:23 INFO ipc.Server: IPC Server handler 5 on 53610: exiting
14/01/02 09:09:23 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:23 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:23 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:23 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:23 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:23 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:23 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:23 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:23 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:23 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:23 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:23 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:23 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:23 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:23 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:23 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:23 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:23 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:23 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:23 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:23 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:23 WARN util.MBeans: Hadoop:service=NameNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1402)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at ShowFileStatusTest.setUp(ShowFileStatusTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:23 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:23 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:23 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:23 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:23 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:23 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:23 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:23 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:23 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:23 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:23 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:23 INFO common.Storage: Number of files = 1
14/01/02 09:09:23 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:23 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:23 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:23 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:24 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:24 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:24 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:24 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:24 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:24 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:24 INFO namenode.FSNamesystem: Finished loading FSImage in 599 msecs
14/01/02 09:09:24 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:24 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:24 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:24 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:24 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:24 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:24 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:24 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 2 msec
14/01/02 09:09:24 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs.
14/01/02 09:09:24 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:24 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:24 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:24 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:24 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:54032
14/01/02 09:09:24 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:24 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:24 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:24 INFO http.HttpServer: listener.getLocalPort() returned 33148 webServer.getConnectors()[0].getLocalPort() returned 33148
14/01/02 09:09:24 INFO http.HttpServer: Jetty bound to port 33148
14/01/02 09:09:24 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:24 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 3 msec
14/01/02 09:09:24 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 3 msec processing time, 3 msec clock time, 1 cycles
14/01/02 09:09:24 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:24 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:24 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_33148_hdfs____.cy7mf5/webapp
14/01/02 09:09:24 INFO mortbay.log: Started SelectChannelConnector@localhost:33148
14/01/02 09:09:24 INFO namenode.NameNode: Web-server up at: localhost:33148
14/01/02 09:09:24 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server listener on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 0 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 5 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 6 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 7 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 1 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 2 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 4 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 3 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 9 on 54032: starting
14/01/02 09:09:24 INFO ipc.Server: IPC Server handler 8 on 54032: starting
14/01/02 09:09:24 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
14/01/02 09:09:24 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at ShowFileStatusTest.setUp(ShowFileStatusTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:24 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:24 INFO common.Storage: Formatting ...
14/01/02 09:09:24 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:24 INFO common.Storage: Formatting ...
14/01/02 09:09:25 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:25 INFO datanode.DataNode: Opened data transfer server at 55441
14/01/02 09:09:25 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:25 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:25 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:25 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:25 INFO http.HttpServer: listener.getLocalPort() returned 42120 webServer.getConnectors()[0].getLocalPort() returned 42120
14/01/02 09:09:25 INFO http.HttpServer: Jetty bound to port 42120
14/01/02 09:09:25 INFO mortbay.log: jetty-6.1.26
14/01/02 09:09:25 INFO mortbay.log: Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_42120_datanode____4go8d4/webapp
14/01/02 09:09:25 INFO mortbay.log: Started SelectChannelConnector@localhost:42120
14/01/02 09:09:25 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:25 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:55441, storageID=, infoPort=42120, ipcPort=55487)
14/01/02 09:09:25 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:55441 storage DS-381669584-127.0.1.1-55441-1388682565275
14/01/02 09:09:25 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:55441
14/01/02 09:09:25 INFO datanode.DataNode: New storage id DS-381669584-127.0.1.1-55441-1388682565275 is assigned to data-node 127.0.0.1:55441
14/01/02 09:09:25 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:25 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:55441 0 blocks
14/01/02 09:09:25 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:25 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:55441, storageID=DS-381669584-127.0.1.1-55441-1388682565275, infoPort=42120, ipcPort=55487)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:25 INFO datanode.DataNode: Finished asynchronous block report scan in 0ms
14/01/02 09:09:25 INFO ipc.Server: IPC Server listener on 55487: starting
14/01/02 09:09:25 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:25 INFO ipc.Server: IPC Server handler 0 on 55487: starting
14/01/02 09:09:25 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:25 INFO ipc.Server: IPC Server handler 1 on 55487: starting
14/01/02 09:09:25 INFO ipc.Server: IPC Server handler 2 on 55487: starting
14/01/02 09:09:25 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:25 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:55441, blocks: 0, processing time: 1 msecs
14/01/02 09:09:25 INFO datanode.DataNode: BlockReport of 0 blocks took 0 msec to generate and 1 msecs for RPC and NN processing
14/01/02 09:09:25 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:25 INFO datanode.DataNode: Generated rough (lockless) block report in 0 ms
14/01/02 09:09:25 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
Cluster is active
14/01/02 09:09:25 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /dir/file. blk_-1737922804228966964_1001
14/01/02 09:09:25 INFO datanode.DataNode: Receiving block blk_-1737922804228966964_1001 src: /127.0.0.1:47140 dest: /127.0.0.1:55441
14/01/02 09:09:25 INFO DataNode.clienttrace: src: /127.0.0.1:47140, dest: /127.0.0.1:55441, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1431861335_1, offset: 0, srvID: DS-381669584-127.0.1.1-55441-1388682565275, blockid: blk_-1737922804228966964_1001, duration: 266256
14/01/02 09:09:25 INFO datanode.DataNode: PacketResponder 0 for block blk_-1737922804228966964_1001 terminating
14/01/02 09:09:25 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55441 is added to blk_-1737922804228966964_1001 size 7
14/01/02 09:09:26 INFO hdfs.StateChange: Removing lease on  file /dir/file from client DFSClient_NONMAPREDUCE_-1431861335_1
14/01/02 09:09:26 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /dir/file is closed by DFSClient_NONMAPREDUCE_-1431861335_1
Shutting down the Mini HDFS Cluster
Shutting down DataNode 0
14/01/02 09:09:26 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:26 INFO ipc.Server: Stopping server on 55487
14/01/02 09:09:26 INFO ipc.Server: IPC Server handler 1 on 55487: exiting
14/01/02 09:09:26 INFO ipc.Server: IPC Server handler 2 on 55487: exiting
14/01/02 09:09:26 INFO ipc.Server: Stopping IPC Server listener on 55487
14/01/02 09:09:26 INFO ipc.Server: IPC Server handler 0 on 55487: exiting
14/01/02 09:09:26 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:26 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 1
14/01/02 09:09:26 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:26 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:55441, storageID=DS-381669584-127.0.1.1-55441-1388682565275, infoPort=42120, ipcPort=55487):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:09:26 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:09:26 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:09:27 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:27 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:55441, storageID=DS-381669584-127.0.1.1-55441-1388682565275, infoPort=42120, ipcPort=55487):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:27 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:27 INFO ipc.Server: Stopping server on 55487
14/01/02 09:09:27 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:27 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:09:27 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:09:27 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:09:27 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId1684544006
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId1684544006
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at ShowFileStatusTest.tearDown(ShowFileStatusTest.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:27 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
14/01/02 09:09:27 INFO mortbay.log: Stopped SelectChannelConnector@localhost:0
14/01/02 09:09:27 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:09:27 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:27 INFO namenode.FSNamesystem: Number of transactions: 5 Total time for transactions(ms): 2Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 138 106 
14/01/02 09:09:27 INFO namenode.FSEditLog: closing edit log: position=453, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:27 INFO namenode.FSEditLog: close success: truncate to 453, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:27 INFO namenode.FSEditLog: closing edit log: position=453, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:27 INFO namenode.FSEditLog: close success: truncate to 453, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:27 INFO ipc.Server: Stopping server on 54032
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 0 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 1 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 2 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 4 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 9 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 5 on 54032: exiting
14/01/02 09:09:27 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:09:27 INFO ipc.Server: Stopping IPC Server listener on 54032
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 3 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 6 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 7 on 54032: exiting
14/01/02 09:09:27 INFO ipc.Server: IPC Server handler 8 on 54032: exiting
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.707 sec
Running FileSystemGlobTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.077 sec

Results :

Tests run: 15, Failures: 0, Errors: 0, Skipped: 1

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch03 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch03/target/ch03-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch03 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch03/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch03 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch03/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch03 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch03/target/ch03-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch03/3.0/ch03-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch03/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch03/3.0/ch03-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 4: Hadoop I/O 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch04 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch04/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch04 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch04 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch04/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch04 ---
[INFO] Compiling 17 source files to /home/pkeni/git/hadoop-book/ch04/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch04 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch04 ---
[INFO] Compiling 19 source files to /home/pkeni/git/hadoop-book/ch04/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch04 ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/ch04/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running VLongWritableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.083 sec
Running MapFileSeekTest
14/01/02 09:09:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new compressor
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new compressor
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:09:30 INFO compress.CodecPool: Got brand-new decompressor
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.773 sec
Running GenericWritableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec
Running BooleanWritableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec
Running ObjectWritableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 sec
Running NullWritableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 sec
Running IntWritableTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec
Running StringTextComparisonTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 sec
Running ArrayWritableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 sec
Running TextTest
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec
Running BytesWritableTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 sec
Running TextPairTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 sec
Running IntPairTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec
Running MapWritableTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec
Running VIntWritableTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec
Running SequenceFileSeekAndSyncTest
[128]	100	One, two, buckle my shoe
[173]	99	Three, four, shut the door
[220]	98	Five, six, pick up sticks
[264]	97	Seven, eight, lay them straight
[314]	96	Nine, ten, a big fat hen
[359]	95	One, two, buckle my shoe
[404]	94	Three, four, shut the door
[451]	93	Five, six, pick up sticks
[495]	92	Seven, eight, lay them straight
[545]	91	Nine, ten, a big fat hen
[590]	90	One, two, buckle my shoe
[635]	89	Three, four, shut the door
[682]	88	Five, six, pick up sticks
[726]	87	Seven, eight, lay them straight
[776]	86	Nine, ten, a big fat hen
[821]	85	One, two, buckle my shoe
[866]	84	Three, four, shut the door
[913]	83	Five, six, pick up sticks
[957]	82	Seven, eight, lay them straight
[1007]	81	Nine, ten, a big fat hen
[1052]	80	One, two, buckle my shoe
[1097]	79	Three, four, shut the door
[1144]	78	Five, six, pick up sticks
[1188]	77	Seven, eight, lay them straight
[1238]	76	Nine, ten, a big fat hen
[1283]	75	One, two, buckle my shoe
[1328]	74	Three, four, shut the door
[1375]	73	Five, six, pick up sticks
[1419]	72	Seven, eight, lay them straight
[1469]	71	Nine, ten, a big fat hen
[1514]	70	One, two, buckle my shoe
[1559]	69	Three, four, shut the door
[1606]	68	Five, six, pick up sticks
[1650]	67	Seven, eight, lay them straight
[1700]	66	Nine, ten, a big fat hen
[1745]	65	One, two, buckle my shoe
[1790]	64	Three, four, shut the door
[1837]	63	Five, six, pick up sticks
[1881]	62	Seven, eight, lay them straight
[1931]	61	Nine, ten, a big fat hen
[1976]	60	One, two, buckle my shoe
[2021]	59	Three, four, shut the door
[2088]	58	Five, six, pick up sticks
[2132]	57	Seven, eight, lay them straight
[2182]	56	Nine, ten, a big fat hen
[2227]	55	One, two, buckle my shoe
[2272]	54	Three, four, shut the door
[2319]	53	Five, six, pick up sticks
[2363]	52	Seven, eight, lay them straight
[2413]	51	Nine, ten, a big fat hen
[2458]	50	One, two, buckle my shoe
[2503]	49	Three, four, shut the door
[2550]	48	Five, six, pick up sticks
[2594]	47	Seven, eight, lay them straight
[2644]	46	Nine, ten, a big fat hen
[2689]	45	One, two, buckle my shoe
[2734]	44	Three, four, shut the door
[2781]	43	Five, six, pick up sticks
[2825]	42	Seven, eight, lay them straight
[2875]	41	Nine, ten, a big fat hen
[2920]	40	One, two, buckle my shoe
[2965]	39	Three, four, shut the door
[3012]	38	Five, six, pick up sticks
[3056]	37	Seven, eight, lay them straight
[3106]	36	Nine, ten, a big fat hen
[3151]	35	One, two, buckle my shoe
[3196]	34	Three, four, shut the door
[3243]	33	Five, six, pick up sticks
[3287]	32	Seven, eight, lay them straight
[3337]	31	Nine, ten, a big fat hen
[3382]	30	One, two, buckle my shoe
[3427]	29	Three, four, shut the door
[3474]	28	Five, six, pick up sticks
[3518]	27	Seven, eight, lay them straight
[3568]	26	Nine, ten, a big fat hen
[3613]	25	One, two, buckle my shoe
[3658]	24	Three, four, shut the door
[3705]	23	Five, six, pick up sticks
[3749]	22	Seven, eight, lay them straight
[3799]	21	Nine, ten, a big fat hen
[3844]	20	One, two, buckle my shoe
[3889]	19	Three, four, shut the door
[3936]	18	Five, six, pick up sticks
[3980]	17	Seven, eight, lay them straight
[4030]	16	Nine, ten, a big fat hen
[4075]	15	One, two, buckle my shoe
[4140]	14	Three, four, shut the door
[4187]	13	Five, six, pick up sticks
[4231]	12	Seven, eight, lay them straight
[4281]	11	Nine, ten, a big fat hen
[4326]	10	One, two, buckle my shoe
[4371]	9	Three, four, shut the door
[4418]	8	Five, six, pick up sticks
[4462]	7	Seven, eight, lay them straight
[4512]	6	Nine, ten, a big fat hen
[4557]	5	One, two, buckle my shoe
[4602]	4	Three, four, shut the door
[4649]	3	Five, six, pick up sticks
[4693]	2	Seven, eight, lay them straight
[4743]	1	Nine, ten, a big fat hen
[128]	100	One, two, buckle my shoe
[173]	99	Three, four, shut the door
[220]	98	Five, six, pick up sticks
[264]	97	Seven, eight, lay them straight
[314]	96	Nine, ten, a big fat hen
[359]	95	One, two, buckle my shoe
[404]	94	Three, four, shut the door
[451]	93	Five, six, pick up sticks
[495]	92	Seven, eight, lay them straight
[545]	91	Nine, ten, a big fat hen
[590]	90	One, two, buckle my shoe
[635]	89	Three, four, shut the door
[682]	88	Five, six, pick up sticks
[726]	87	Seven, eight, lay them straight
[776]	86	Nine, ten, a big fat hen
[821]	85	One, two, buckle my shoe
[866]	84	Three, four, shut the door
[913]	83	Five, six, pick up sticks
[957]	82	Seven, eight, lay them straight
[1007]	81	Nine, ten, a big fat hen
[1052]	80	One, two, buckle my shoe
[1097]	79	Three, four, shut the door
[1144]	78	Five, six, pick up sticks
[1188]	77	Seven, eight, lay them straight
[1238]	76	Nine, ten, a big fat hen
[1283]	75	One, two, buckle my shoe
[1328]	74	Three, four, shut the door
[1375]	73	Five, six, pick up sticks
[1419]	72	Seven, eight, lay them straight
[1469]	71	Nine, ten, a big fat hen
[1514]	70	One, two, buckle my shoe
[1559]	69	Three, four, shut the door
[1606]	68	Five, six, pick up sticks
[1650]	67	Seven, eight, lay them straight
[1700]	66	Nine, ten, a big fat hen
[1745]	65	One, two, buckle my shoe
[1790]	64	Three, four, shut the door
[1837]	63	Five, six, pick up sticks
[1881]	62	Seven, eight, lay them straight
[1931]	61	Nine, ten, a big fat hen
[1976]	60	One, two, buckle my shoe
[2021]	59	Three, four, shut the door
[2088]	58	Five, six, pick up sticks
[2132]	57	Seven, eight, lay them straight
[2182]	56	Nine, ten, a big fat hen
[2227]	55	One, two, buckle my shoe
[2272]	54	Three, four, shut the door
[2319]	53	Five, six, pick up sticks
[2363]	52	Seven, eight, lay them straight
[2413]	51	Nine, ten, a big fat hen
[2458]	50	One, two, buckle my shoe
[2503]	49	Three, four, shut the door
[2550]	48	Five, six, pick up sticks
[2594]	47	Seven, eight, lay them straight
[2644]	46	Nine, ten, a big fat hen
[2689]	45	One, two, buckle my shoe
[2734]	44	Three, four, shut the door
[2781]	43	Five, six, pick up sticks
[2825]	42	Seven, eight, lay them straight
[2875]	41	Nine, ten, a big fat hen
[2920]	40	One, two, buckle my shoe
[2965]	39	Three, four, shut the door
[3012]	38	Five, six, pick up sticks
[3056]	37	Seven, eight, lay them straight
[3106]	36	Nine, ten, a big fat hen
[3151]	35	One, two, buckle my shoe
[3196]	34	Three, four, shut the door
[3243]	33	Five, six, pick up sticks
[3287]	32	Seven, eight, lay them straight
[3337]	31	Nine, ten, a big fat hen
[3382]	30	One, two, buckle my shoe
[3427]	29	Three, four, shut the door
[3474]	28	Five, six, pick up sticks
[3518]	27	Seven, eight, lay them straight
[3568]	26	Nine, ten, a big fat hen
[3613]	25	One, two, buckle my shoe
[3658]	24	Three, four, shut the door
[3705]	23	Five, six, pick up sticks
[3749]	22	Seven, eight, lay them straight
[3799]	21	Nine, ten, a big fat hen
[3844]	20	One, two, buckle my shoe
[3889]	19	Three, four, shut the door
[3936]	18	Five, six, pick up sticks
[3980]	17	Seven, eight, lay them straight
[4030]	16	Nine, ten, a big fat hen
[4075]	15	One, two, buckle my shoe
[4140]	14	Three, four, shut the door
[4187]	13	Five, six, pick up sticks
[4231]	12	Seven, eight, lay them straight
[4281]	11	Nine, ten, a big fat hen
[4326]	10	One, two, buckle my shoe
[4371]	9	Three, four, shut the door
[4418]	8	Five, six, pick up sticks
[4462]	7	Seven, eight, lay them straight
[4512]	6	Nine, ten, a big fat hen
[4557]	5	One, two, buckle my shoe
[4602]	4	Three, four, shut the door
[4649]	3	Five, six, pick up sticks
[4693]	2	Seven, eight, lay them straight
[4743]	1	Nine, ten, a big fat hen
[128]	100	One, two, buckle my shoe
[173]	99	Three, four, shut the door
[220]	98	Five, six, pick up sticks
[264]	97	Seven, eight, lay them straight
[314]	96	Nine, ten, a big fat hen
[359]	95	One, two, buckle my shoe
[404]	94	Three, four, shut the door
[451]	93	Five, six, pick up sticks
[495]	92	Seven, eight, lay them straight
[545]	91	Nine, ten, a big fat hen
[590]	90	One, two, buckle my shoe
[635]	89	Three, four, shut the door
[682]	88	Five, six, pick up sticks
[726]	87	Seven, eight, lay them straight
[776]	86	Nine, ten, a big fat hen
[821]	85	One, two, buckle my shoe
[866]	84	Three, four, shut the door
[913]	83	Five, six, pick up sticks
[957]	82	Seven, eight, lay them straight
[1007]	81	Nine, ten, a big fat hen
[1052]	80	One, two, buckle my shoe
[1097]	79	Three, four, shut the door
[1144]	78	Five, six, pick up sticks
[1188]	77	Seven, eight, lay them straight
[1238]	76	Nine, ten, a big fat hen
[1283]	75	One, two, buckle my shoe
[1328]	74	Three, four, shut the door
[1375]	73	Five, six, pick up sticks
[1419]	72	Seven, eight, lay them straight
[1469]	71	Nine, ten, a big fat hen
[1514]	70	One, two, buckle my shoe
[1559]	69	Three, four, shut the door
[1606]	68	Five, six, pick up sticks
[1650]	67	Seven, eight, lay them straight
[1700]	66	Nine, ten, a big fat hen
[1745]	65	One, two, buckle my shoe
[1790]	64	Three, four, shut the door
[1837]	63	Five, six, pick up sticks
[1881]	62	Seven, eight, lay them straight
[1931]	61	Nine, ten, a big fat hen
[1976]	60	One, two, buckle my shoe
[2021]	59	Three, four, shut the door
[2088]	58	Five, six, pick up sticks
[2132]	57	Seven, eight, lay them straight
[2182]	56	Nine, ten, a big fat hen
[2227]	55	One, two, buckle my shoe
[2272]	54	Three, four, shut the door
[2319]	53	Five, six, pick up sticks
[2363]	52	Seven, eight, lay them straight
[2413]	51	Nine, ten, a big fat hen
[2458]	50	One, two, buckle my shoe
[2503]	49	Three, four, shut the door
[2550]	48	Five, six, pick up sticks
[2594]	47	Seven, eight, lay them straight
[2644]	46	Nine, ten, a big fat hen
[2689]	45	One, two, buckle my shoe
[2734]	44	Three, four, shut the door
[2781]	43	Five, six, pick up sticks
[2825]	42	Seven, eight, lay them straight
[2875]	41	Nine, ten, a big fat hen
[2920]	40	One, two, buckle my shoe
[2965]	39	Three, four, shut the door
[3012]	38	Five, six, pick up sticks
[3056]	37	Seven, eight, lay them straight
[3106]	36	Nine, ten, a big fat hen
[3151]	35	One, two, buckle my shoe
[3196]	34	Three, four, shut the door
[3243]	33	Five, six, pick up sticks
[3287]	32	Seven, eight, lay them straight
[3337]	31	Nine, ten, a big fat hen
[3382]	30	One, two, buckle my shoe
[3427]	29	Three, four, shut the door
[3474]	28	Five, six, pick up sticks
[3518]	27	Seven, eight, lay them straight
[3568]	26	Nine, ten, a big fat hen
[3613]	25	One, two, buckle my shoe
[3658]	24	Three, four, shut the door
[3705]	23	Five, six, pick up sticks
[3749]	22	Seven, eight, lay them straight
[3799]	21	Nine, ten, a big fat hen
[3844]	20	One, two, buckle my shoe
[3889]	19	Three, four, shut the door
[3936]	18	Five, six, pick up sticks
[3980]	17	Seven, eight, lay them straight
[4030]	16	Nine, ten, a big fat hen
[4075]	15	One, two, buckle my shoe
[4140]	14	Three, four, shut the door
[4187]	13	Five, six, pick up sticks
[4231]	12	Seven, eight, lay them straight
[4281]	11	Nine, ten, a big fat hen
[4326]	10	One, two, buckle my shoe
[4371]	9	Three, four, shut the door
[4418]	8	Five, six, pick up sticks
[4462]	7	Seven, eight, lay them straight
[4512]	6	Nine, ten, a big fat hen
[4557]	5	One, two, buckle my shoe
[4602]	4	Three, four, shut the door
[4649]	3	Five, six, pick up sticks
[4693]	2	Seven, eight, lay them straight
[4743]	1	Nine, ten, a big fat hen
[128]	100	One, two, buckle my shoe
[173]	99	Three, four, shut the door
[220]	98	Five, six, pick up sticks
[264]	97	Seven, eight, lay them straight
[314]	96	Nine, ten, a big fat hen
[359]	95	One, two, buckle my shoe
[404]	94	Three, four, shut the door
[451]	93	Five, six, pick up sticks
[495]	92	Seven, eight, lay them straight
[545]	91	Nine, ten, a big fat hen
[590]	90	One, two, buckle my shoe
[635]	89	Three, four, shut the door
[682]	88	Five, six, pick up sticks
[726]	87	Seven, eight, lay them straight
[776]	86	Nine, ten, a big fat hen
[821]	85	One, two, buckle my shoe
[866]	84	Three, four, shut the door
[913]	83	Five, six, pick up sticks
[957]	82	Seven, eight, lay them straight
[1007]	81	Nine, ten, a big fat hen
[1052]	80	One, two, buckle my shoe
[1097]	79	Three, four, shut the door
[1144]	78	Five, six, pick up sticks
[1188]	77	Seven, eight, lay them straight
[1238]	76	Nine, ten, a big fat hen
[1283]	75	One, two, buckle my shoe
[1328]	74	Three, four, shut the door
[1375]	73	Five, six, pick up sticks
[1419]	72	Seven, eight, lay them straight
[1469]	71	Nine, ten, a big fat hen
[1514]	70	One, two, buckle my shoe
[1559]	69	Three, four, shut the door
[1606]	68	Five, six, pick up sticks
[1650]	67	Seven, eight, lay them straight
[1700]	66	Nine, ten, a big fat hen
[1745]	65	One, two, buckle my shoe
[1790]	64	Three, four, shut the door
[1837]	63	Five, six, pick up sticks
[1881]	62	Seven, eight, lay them straight
[1931]	61	Nine, ten, a big fat hen
[1976]	60	One, two, buckle my shoe
[2021]	59	Three, four, shut the door
[2088]	58	Five, six, pick up sticks
[2132]	57	Seven, eight, lay them straight
[2182]	56	Nine, ten, a big fat hen
[2227]	55	One, two, buckle my shoe
[2272]	54	Three, four, shut the door
[2319]	53	Five, six, pick up sticks
[2363]	52	Seven, eight, lay them straight
[2413]	51	Nine, ten, a big fat hen
[2458]	50	One, two, buckle my shoe
[2503]	49	Three, four, shut the door
[2550]	48	Five, six, pick up sticks
[2594]	47	Seven, eight, lay them straight
[2644]	46	Nine, ten, a big fat hen
[2689]	45	One, two, buckle my shoe
[2734]	44	Three, four, shut the door
[2781]	43	Five, six, pick up sticks
[2825]	42	Seven, eight, lay them straight
[2875]	41	Nine, ten, a big fat hen
[2920]	40	One, two, buckle my shoe
[2965]	39	Three, four, shut the door
[3012]	38	Five, six, pick up sticks
[3056]	37	Seven, eight, lay them straight
[3106]	36	Nine, ten, a big fat hen
[3151]	35	One, two, buckle my shoe
[3196]	34	Three, four, shut the door
[3243]	33	Five, six, pick up sticks
[3287]	32	Seven, eight, lay them straight
[3337]	31	Nine, ten, a big fat hen
[3382]	30	One, two, buckle my shoe
[3427]	29	Three, four, shut the door
[3474]	28	Five, six, pick up sticks
[3518]	27	Seven, eight, lay them straight
[3568]	26	Nine, ten, a big fat hen
[3613]	25	One, two, buckle my shoe
[3658]	24	Three, four, shut the door
[3705]	23	Five, six, pick up sticks
[3749]	22	Seven, eight, lay them straight
[3799]	21	Nine, ten, a big fat hen
[3844]	20	One, two, buckle my shoe
[3889]	19	Three, four, shut the door
[3936]	18	Five, six, pick up sticks
[3980]	17	Seven, eight, lay them straight
[4030]	16	Nine, ten, a big fat hen
[4075]	15	One, two, buckle my shoe
[4140]	14	Three, four, shut the door
[4187]	13	Five, six, pick up sticks
[4231]	12	Seven, eight, lay them straight
[4281]	11	Nine, ten, a big fat hen
[4326]	10	One, two, buckle my shoe
[4371]	9	Three, four, shut the door
[4418]	8	Five, six, pick up sticks
[4462]	7	Seven, eight, lay them straight
[4512]	6	Nine, ten, a big fat hen
[4557]	5	One, two, buckle my shoe
[4602]	4	Three, four, shut the door
[4649]	3	Five, six, pick up sticks
[4693]	2	Seven, eight, lay them straight
[4743]	1	Nine, ten, a big fat hen
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.297 sec
Running FileDecompressorTest
14/01/02 09:09:31 WARN snappy.LoadSnappy: Snappy native library not loaded
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 sec

Results :

Tests run: 34, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch04 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch04/target/ch04-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch04 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch04/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch04 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch04/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch04 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch04/target/ch04-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch04/3.0/ch04-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch04/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch04/3.0/ch04-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 5: Developing a MapReduce Application 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch05 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch05/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch05 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch05 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch05 ---
[INFO] Compiling 18 source files to /home/pkeni/git/hadoop-book/ch05/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch05 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch05 ---
[INFO] Compiling 9 source files to /home/pkeni/git/hadoop-book/ch05/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch05 ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/ch05/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running v1.MaxTemperatureMapperTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.55 sec
Running MultipleResourceConfigurationTest
14/01/02 09:09:33 WARN conf.Configuration: configuration-2.xml:a attempt to override final parameter: weight;  Ignoring.
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.062 sec
Running v1.MaxTemperatureReducerTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.135 sec
Running v3.MaxTemperatureDriverTest
14/01/02 09:09:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/01/02 09:09:33 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/01/02 09:09:33 WARN mapred.JobClient: No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).
14/01/02 09:09:33 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:09:33 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:09:34 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:09:34 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:09:34 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@6033baf5
14/01/02 09:09:34 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:09:34 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:09:34 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:09:34 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:09:34 INFO mapred.MapTask: Finished spill 0
14/01/02 09:09:34 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:09:34 INFO mapred.LocalJobRunner: 
14/01/02 09:09:34 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:09:34 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@3b5b8981
14/01/02 09:09:34 INFO mapred.LocalJobRunner: 
14/01/02 09:09:34 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:09:34 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24 bytes
14/01/02 09:09:34 INFO mapred.LocalJobRunner: 
14/01/02 09:09:34 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:09:34 INFO mapred.LocalJobRunner: 
14/01/02 09:09:34 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:09:34 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:09:34 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:09:34 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:09:35 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:09:35 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:09:35 INFO mapred.JobClient: Counters: 20
14/01/02 09:09:35 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:09:35 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:09:35 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:09:35 INFO mapred.JobClient:     FILE_BYTES_READ=1458
14/01/02 09:09:35 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=69315
14/01/02 09:09:35 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:09:35 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:09:35 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:09:35 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:09:35 INFO mapred.JobClient:     Map output materialized bytes=28
14/01/02 09:09:35 INFO mapred.JobClient:     Combine output records=2
14/01/02 09:09:35 INFO mapred.JobClient:     Map input records=5
14/01/02 09:09:35 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:09:35 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:09:35 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:09:35 INFO mapred.JobClient:     Spilled Records=4
14/01/02 09:09:35 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:09:35 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:09:35 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:09:35 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:09:35 INFO mapred.JobClient:     Combine input records=5
14/01/02 09:09:35 INFO mapred.JobClient:     Map output records=5
14/01/02 09:09:35 INFO mapred.JobClient:     SPLIT_RAW_BYTES=130
14/01/02 09:09:35 INFO mapred.JobClient:     Reduce input records=2
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.592 sec
Running v5.MaxTemperatureMapperTest
Ignoring possibly corrupt input: 0335999999433181957042302005+37950+139117SAO  +0004RJSN V02011359003150070356999999433201957010100005+353
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.033 sec
Running v3.MaxTemperatureDriverMiniTest
14/01/02 09:09:35 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:35 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:35 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:35 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:35 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:35 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:35 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:35 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:35 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:35 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:35 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:35 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:35 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:35 INFO common.Storage: Storage directory /tmp/dfs/name1 has been successfully formatted.
14/01/02 09:09:35 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:35 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:35 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:36 INFO common.Storage: Storage directory /tmp/dfs/name2 has been successfully formatted.
14/01/02 09:09:36 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-namenode.properties, hadoop-metrics2.properties
14/01/02 09:09:36 INFO util.GSet: VM type       = 64-bit
14/01/02 09:09:36 INFO util.GSet: 2% max memory = 18.21 MB
14/01/02 09:09:36 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/01/02 09:09:36 INFO util.GSet: recommended=2097152, actual=2097152
14/01/02 09:09:36 INFO namenode.FSNamesystem: fsOwner=pkeni
14/01/02 09:09:36 INFO namenode.FSNamesystem: supergroup=supergroup
14/01/02 09:09:36 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/01/02 09:09:36 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/01/02 09:09:36 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/01/02 09:09:36 INFO namenode.FSNamesystem: Registered FSNamesystemStateMBean and NameNodeMXBean
14/01/02 09:09:36 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/01/02 09:09:36 INFO common.Storage: Number of files = 1
14/01/02 09:09:36 INFO common.Storage: Number of files under construction = 0
14/01/02 09:09:36 INFO common.Storage: Image file of size 111 loaded in 0 seconds.
14/01/02 09:09:36 INFO common.Storage: Edits file /tmp/dfs/name1/current/edits of size 4 edits # 0 loaded in 0 seconds.
14/01/02 09:09:36 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:36 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:36 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:09:36 INFO common.Storage: Image file of size 111 saved in 0 seconds.
14/01/02 09:09:36 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:36 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:09:37 INFO namenode.NameCache: initialized with 0 entries 0 lookups
14/01/02 09:09:37 INFO namenode.FSNamesystem: Finished loading FSImage in 1111 msecs
14/01/02 09:09:37 INFO namenode.FSNamesystem: dfs.safemode.threshold.pct          = 0.9990000128746033
14/01/02 09:09:37 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
14/01/02 09:09:37 INFO namenode.FSNamesystem: dfs.safemode.extension              = 0
14/01/02 09:09:37 INFO namenode.FSNamesystem: Total number of blocks = 0
14/01/02 09:09:37 INFO namenode.FSNamesystem: Number of invalid blocks = 0
14/01/02 09:09:37 INFO namenode.FSNamesystem: Number of under-replicated blocks = 0
14/01/02 09:09:37 INFO namenode.FSNamesystem: Number of  over-replicated blocks = 0
14/01/02 09:09:37 INFO hdfs.StateChange: STATE* Safe mode termination scan for invalid, over- and under-replicated blocks completed in 8 msec
14/01/02 09:09:37 INFO hdfs.StateChange: STATE* Leaving safe mode after 1 secs.
14/01/02 09:09:37 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
14/01/02 09:09:37 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
14/01/02 09:09:37 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:37 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:37 INFO namenode.FSNamesystem: ReplicateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:37 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: First cycle completed 0 blocks in 0 msec
14/01/02 09:09:37 INFO namenode.FSNamesystem: InvalidateQueue QueueProcessingStatistics: Queue flush completed 0 blocks in 0 msec processing time, 0 msec clock time, 1 cycles
14/01/02 09:09:37 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:37 INFO namenode.NameNode: Namenode up at: localhost/127.0.0.1:34622
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
2014-01-02 09:09:37.450:INFO::Logging to STDERR via org.mortbay.log.StdErrLog
14/01/02 09:09:37 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:37 INFO http.HttpServer: dfs.webhdfs.enabled = false
14/01/02 09:09:37 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:37 INFO http.HttpServer: listener.getLocalPort() returned 34773 webServer.getConnectors()[0].getLocalPort() returned 34773
14/01/02 09:09:37 INFO http.HttpServer: Jetty bound to port 34773
2014-01-02 09:09:37.509:INFO::jetty-6.1.26
2014-01-02 09:09:37.546:INFO::Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/hdfs to /tmp/Jetty_localhost_34773_hdfs____lm5wqk/webapp
2014-01-02 09:09:37.775:INFO::Started SelectChannelConnector@localhost:34773
14/01/02 09:09:37 INFO namenode.NameNode: Web-server up at: localhost:34773
14/01/02 09:09:37 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server listener on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 0 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 1 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 3 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 4 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 2 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 6 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 5 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 9 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 8 on 34622: starting
14/01/02 09:09:37 INFO ipc.Server: IPC Server handler 7 on 34622: starting
Starting DataNode 0 with dfs.data.dir: /tmp/dfs/data/data1,/tmp/dfs/data/data2
14/01/02 09:09:37 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:37 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.startCluster(ClusterMapReduceTestCase.java:102)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.startCluster(ClusterMapReduceTestCase.java:75)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.setUp(ClusterMapReduceTestCase.java:56)
	at v3.MaxTemperatureDriverMiniTest.setUp(MaxTemperatureDriverMiniTest.java:34)
	at junit.framework.TestCase.runBare(TestCase.java:132)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:37 INFO common.Storage: Storage directory /tmp/dfs/data/data1 is not formatted.
14/01/02 09:09:37 INFO common.Storage: Formatting ...
14/01/02 09:09:38 INFO common.Storage: Storage directory /tmp/dfs/data/data2 is not formatted.
14/01/02 09:09:38 INFO common.Storage: Formatting ...
14/01/02 09:09:38 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:38 INFO datanode.DataNode: Opened data transfer server at 55965
14/01/02 09:09:38 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:38 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:38 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:38 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:38 INFO http.HttpServer: listener.getLocalPort() returned 42783 webServer.getConnectors()[0].getLocalPort() returned 42783
14/01/02 09:09:38 INFO http.HttpServer: Jetty bound to port 42783
2014-01-02 09:09:38.392:INFO::jetty-6.1.26
2014-01-02 09:09:38.405:INFO::Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_42783_datanode____iave3f/webapp
2014-01-02 09:09:38.539:INFO::Started SelectChannelConnector@localhost:42783
14/01/02 09:09:38 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:38 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:55965, storageID=, infoPort=42783, ipcPort=59585)
14/01/02 09:09:38 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:55965 storage DS-1988144180-127.0.1.1-55965-1388682578550
14/01/02 09:09:38 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:55965
14/01/02 09:09:38 INFO datanode.DataNode: New storage id DS-1988144180-127.0.1.1-55965-1388682578550 is assigned to data-node 127.0.0.1:55965
14/01/02 09:09:38 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:38 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:55965 0 blocks
14/01/02 09:09:38 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:38 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:55965, storageID=DS-1988144180-127.0.1.1-55965-1388682578550, infoPort=42783, ipcPort=59585)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:09:38 INFO datanode.DataNode: Finished asynchronous block report scan in 0ms
14/01/02 09:09:38 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:38 INFO ipc.Server: IPC Server listener on 59585: starting
14/01/02 09:09:38 INFO ipc.Server: IPC Server handler 2 on 59585: starting
Starting DataNode 1 with dfs.data.dir: /tmp/dfs/data/data3,/tmp/dfs/data/data4
14/01/02 09:09:38 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:38 INFO ipc.Server: IPC Server handler 1 on 59585: starting
14/01/02 09:09:38 INFO ipc.Server: IPC Server handler 0 on 59585: starting
14/01/02 09:09:38 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
14/01/02 09:09:38 WARN util.MBeans: Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1589)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.startCluster(ClusterMapReduceTestCase.java:102)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.startCluster(ClusterMapReduceTestCase.java:75)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.setUp(ClusterMapReduceTestCase.java:56)
	at v3.MaxTemperatureDriverMiniTest.setUp(MaxTemperatureDriverMiniTest.java:34)
	at junit.framework.TestCase.runBare(TestCase.java:132)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:38 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:38 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:55965, blocks: 0, processing time: 1 msecs
14/01/02 09:09:38 INFO datanode.DataNode: BlockReport of 0 blocks took 0 msec to generate and 3 msecs for RPC and NN processing
14/01/02 09:09:38 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:38 INFO datanode.DataNode: Generated rough (lockless) block report in 3 ms
14/01/02 09:09:38 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
14/01/02 09:09:38 INFO common.Storage: Storage directory /tmp/dfs/data/data3 is not formatted.
14/01/02 09:09:38 INFO common.Storage: Formatting ...
14/01/02 09:09:38 INFO common.Storage: Storage directory /tmp/dfs/data/data4 is not formatted.
14/01/02 09:09:38 INFO common.Storage: Formatting ...
14/01/02 09:09:39 INFO datanode.DataNode: Registered FSDatasetStatusMBean
14/01/02 09:09:39 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceAlreadyExistsException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.registerMXBean(DataNode.java:554)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:407)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:309)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1651)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1590)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1565)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:421)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:284)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.startCluster(ClusterMapReduceTestCase.java:102)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.startCluster(ClusterMapReduceTestCase.java:75)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.setUp(ClusterMapReduceTestCase.java:56)
	at v3.MaxTemperatureDriverMiniTest.setUp(MaxTemperatureDriverMiniTest.java:34)
	at junit.framework.TestCase.runBare(TestCase.java:132)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:09:39 INFO datanode.DataNode: Opened data transfer server at 45383
14/01/02 09:09:39 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s
14/01/02 09:09:39 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:39 INFO datanode.DataNode: dfs.webhdfs.enabled = false
14/01/02 09:09:39 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:39 INFO http.HttpServer: listener.getLocalPort() returned 39393 webServer.getConnectors()[0].getLocalPort() returned 39393
14/01/02 09:09:39 INFO http.HttpServer: Jetty bound to port 39393
2014-01-02 09:09:39.162:INFO::jetty-6.1.26
2014-01-02 09:09:39.176:INFO::Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/datanode to /tmp/Jetty_localhost_39393_datanode____.t7kegi/webapp
2014-01-02 09:09:39.293:INFO::Started SelectChannelConnector@localhost:39393
14/01/02 09:09:39 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:39 INFO datanode.DataNode: dnRegistration = DatanodeRegistration(127.0.0.1:45383, storageID=, infoPort=39393, ipcPort=33978)
14/01/02 09:09:39 INFO hdfs.StateChange: BLOCK* NameSystem.registerDatanode: node registration from 127.0.0.1:45383 storage DS-465270554-127.0.1.1-45383-1388682579294
14/01/02 09:09:39 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:45383
14/01/02 09:09:39 INFO datanode.DataNode: New storage id DS-465270554-127.0.1.1-45383-1388682579294 is assigned to data-node 127.0.0.1:45383
14/01/02 09:09:39 INFO datanode.DataNode: Finished generating blocks being written report for 2 volumes in 0 seconds
14/01/02 09:09:39 INFO hdfs.StateChange: *BLOCK* NameNode.blocksBeingWrittenReport: from 127.0.0.1:45383 0 blocks
14/01/02 09:09:39 INFO datanode.DataNode: Starting asynchronous block report scan
14/01/02 09:09:39 INFO datanode.DataNode: Finished asynchronous block report scan in 0ms
14/01/02 09:09:39 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:45383, storageID=DS-465270554-127.0.1.1-45383-1388682579294, infoPort=39393, ipcPort=33978)In DataNode.run, data = FSDataset{dirpath='/tmp/dfs/data/data3/current,/tmp/dfs/data/data4/current'}
14/01/02 09:09:39 INFO ipc.Server: IPC Server listener on 33978: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 0 on 33978: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 1 on 33978: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 2 on 33978: starting
14/01/02 09:09:39 INFO datanode.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 0msec
14/01/02 09:09:39 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 1 ms
14/01/02 09:09:39 INFO hdfs.StateChange: *BLOCK* NameSystem.processReport: from 127.0.0.1:45383, blocks: 0, processing time: 1 msecs
14/01/02 09:09:39 INFO datanode.DataNode: BlockReport of 0 blocks took 1 msec to generate and 7 msecs for RPC and NN processing
14/01/02 09:09:39 INFO datanode.DataNode: Starting Periodic block scanner.
14/01/02 09:09:39 INFO datanode.DataNode: Generated rough (lockless) block report in 1 ms
14/01/02 09:09:39 INFO datanode.DataNode: Reconciled asynchronous block report against current state in 0 ms
Cluster is active
Generating rack names for tasktrackers
Generating host names for tasktrackers
14/01/02 09:09:39 WARN fs.FileSystem: "localhost:34622" is a deprecated filesystem name. Use "hdfs://localhost:34622/" instead.
14/01/02 09:09:39 WARN impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties, hadoop-metrics2.properties
14/01/02 09:09:39 WARN util.MBeans: Hadoop:service=JobTracker,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:151)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:160)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:173)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:930)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:307)
	at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:302)
	at org.apache.hadoop.mapred.MiniMRCluster$JobTrackerRunner$1.run(MiniMRCluster.java:114)
	at org.apache.hadoop.mapred.MiniMRCluster$JobTrackerRunner$1.run(MiniMRCluster.java:112)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)
	at org.apache.hadoop.mapred.MiniMRCluster$JobTrackerRunner.run(MiniMRCluster.java:112)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:09:39 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
14/01/02 09:09:39 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
14/01/02 09:09:39 INFO mapred.JobTracker: Scheduler configured with (memSizeForMapSlotOnJT, memSizeForReduceSlotOnJT, limitMaxMemForMapTasks, limitMaxMemForReduceTasks) (-1, -1, -1, -1)
14/01/02 09:09:39 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
14/01/02 09:09:39 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:39 INFO mapred.JobTracker: Starting jobtracker with owner as pkeni
14/01/02 09:09:39 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:39 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:39 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:39 INFO http.HttpServer: listener.getLocalPort() returned 58883 webServer.getConnectors()[0].getLocalPort() returned 58883
14/01/02 09:09:39 INFO http.HttpServer: Jetty bound to port 58883
2014-01-02 09:09:39.400:INFO::jetty-6.1.26
2014-01-02 09:09:39.415:INFO::Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/job to /tmp/Jetty_localhost_58883_job____9vgvku/webapp
2014-01-02 09:09:39.555:INFO::Started SelectChannelConnector@localhost:58883
14/01/02 09:09:39 INFO mapred.JobTracker: JobTracker up at: 46232
14/01/02 09:09:39 INFO mapred.JobTracker: JobTracker webserver: 58883
14/01/02 09:09:39 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server listener on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 0 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 1 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 2 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 3 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 4 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 5 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 6 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 7 on 46232: starting
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 8 on 46232: starting
14/01/02 09:09:39 INFO mapred.JobTracker: Setting safe mode to true. Requested by : pkeni
14/01/02 09:09:39 INFO ipc.Server: IPC Server handler 9 on 46232: starting
14/01/02 09:09:39 INFO mapred.JobTracker: Setting safe mode to false. Requested by : pkeni
14/01/02 09:09:39 INFO mapred.JobTracker: Cleaning up the system directory
14/01/02 09:09:39 INFO mapred.JobTracker: History server being initialized in embedded mode
14/01/02 09:09:39 INFO mapred.JobHistoryServer: Started job history server at: localhost:58883
14/01/02 09:09:39 INFO mapred.JobTracker: Job History Server web address: localhost:58883
14/01/02 09:09:39 INFO mapred.CompletedJobStatusStore: Completed job store is inactive
14/01/02 09:09:39 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /tmp/hadoop-pkeni/mapred/system/jobtracker.info. blk_4539425945695071007_1001
14/01/02 09:09:39 INFO datanode.DataNode: Receiving block blk_4539425945695071007_1001 src: /127.0.0.1:49918 dest: /127.0.0.1:55965
14/01/02 09:09:39 INFO datanode.DataNode: Receiving block blk_4539425945695071007_1001 src: /127.0.0.1:56796 dest: /127.0.0.1:45383
14/01/02 09:09:39 INFO DataNode.clienttrace: src: /127.0.0.1:56796, dest: /127.0.0.1:45383, bytes: 4, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_4539425945695071007_1001, duration: 6843120
14/01/02 09:09:39 INFO datanode.DataNode: PacketResponder 0 for block blk_4539425945695071007_1001 terminating
14/01/02 09:09:39 INFO DataNode.clienttrace: src: /127.0.0.1:49918, dest: /127.0.0.1:55965, bytes: 4, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_4539425945695071007_1001, duration: 5250557
14/01/02 09:09:39 INFO datanode.DataNode: PacketResponder 1 for block blk_4539425945695071007_1001 terminating
14/01/02 09:09:39 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_4539425945695071007_1001 size 4
14/01/02 09:09:39 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_4539425945695071007_1001 size 4
14/01/02 09:09:40 INFO mapred.MiniMRCluster: JobTracker still initializing. Waiting.
14/01/02 09:09:40 INFO hdfs.StateChange: Removing lease on  file /tmp/hadoop-pkeni/mapred/system/jobtracker.info from client DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:40 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /tmp/hadoop-pkeni/mapred/system/jobtracker.info is closed by DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:40 INFO mapred.JobTracker: Starting the recovery process for 0 jobs ...
14/01/02 09:09:40 INFO mapred.JobTracker: Recovery done! Recoverd 0 of 0 jobs.
14/01/02 09:09:40 INFO mapred.JobTracker: Recovery Duration (ms):1
14/01/02 09:09:40 INFO mapred.JobTracker: Refreshing hosts information
14/01/02 09:09:40 INFO util.HostsFileReader: Setting the includes file to 
14/01/02 09:09:40 INFO util.HostsFileReader: Setting the excludes file to 
14/01/02 09:09:40 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
14/01/02 09:09:40 INFO mapred.JobTracker: Decommissioning 0 nodes
14/01/02 09:09:40 INFO mapred.JobTracker: Starting RUNNING
14/01/02 09:09:41 WARN fs.FileSystem: "localhost:34622" is a deprecated filesystem name. Use "hdfs://localhost:34622/" instead.
14/01/02 09:09:41 INFO mapred.MiniMRCluster: mapred.local.dir is /tmp/hadoop-pkeni/mapred/local/0_0
14/01/02 09:09:46 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:46 INFO mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1
14/01/02 09:09:46 INFO mapred.TaskTracker: Starting tasktracker with owner as pkeni
14/01/02 09:09:46 INFO mapred.TaskTracker: Good mapred local directories are: /tmp/hadoop-pkeni/mapred/local/0_0
14/01/02 09:09:46 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:46 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:46 INFO ipc.Server: IPC Server listener on 33030: starting
14/01/02 09:09:46 INFO ipc.Server: IPC Server handler 0 on 33030: starting
14/01/02 09:09:46 INFO ipc.Server: IPC Server handler 1 on 33030: starting
14/01/02 09:09:46 INFO ipc.Server: IPC Server handler 2 on 33030: starting
14/01/02 09:09:46 INFO ipc.Server: IPC Server handler 3 on 33030: starting
14/01/02 09:09:46 INFO mapred.TaskTracker: TaskTracker up at: localhost/127.0.0.1:33030
14/01/02 09:09:46 INFO mapred.TaskTracker: Starting tracker tracker_host0.foo.com:localhost/127.0.0.1:33030
14/01/02 09:09:46 INFO mapred.TaskTracker:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@3fe2afac
14/01/02 09:09:46 INFO mapred.TaskTracker: Starting thread: Map-events fetcher for all reduce tasks on tracker_host0.foo.com:localhost/127.0.0.1:33030
14/01/02 09:09:46 WARN mapred.TaskTracker: TaskTracker's totalMemoryAllottedForTasks is -1. TaskMemoryManager is disabled.
14/01/02 09:09:46 INFO mapred.IndexCache: IndexCache created with max memory = 10485760
14/01/02 09:09:46 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:46 INFO http.HttpServer: listener.getLocalPort() returned 35016 webServer.getConnectors()[0].getLocalPort() returned 35016
14/01/02 09:09:46 INFO http.HttpServer: Jetty bound to port 35016
2014-01-02 09:09:46.427:INFO::jetty-6.1.26
2014-01-02 09:09:46.447:INFO::Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/task to /tmp/Jetty_0_0_0_0_35016_task____.3j61wv/webapp
2014-01-02 09:09:46.579:INFO::Started SelectChannelConnector@0.0.0.0:35016
14/01/02 09:09:46 INFO mapred.TaskTracker: FILE_CACHE_SIZE for mapOutputServlet set to : 2000
14/01/02 09:09:46 WARN fs.FileSystem: "localhost:34622" is a deprecated filesystem name. Use "hdfs://localhost:34622/" instead.
14/01/02 09:09:46 INFO mapred.MiniMRCluster: mapred.local.dir is /tmp/hadoop-pkeni/mapred/local/1_0
14/01/02 09:09:46 INFO mapred.UserLogCleaner: Adding job_20140102083937430_0001 for user-log deletion with retainTimeStamp:1388768986418
14/01/02 09:09:46 INFO net.NetworkTopology: Adding a new node: /default-rack/host0.foo.com
14/01/02 09:09:46 INFO mapred.JobTracker: Adding tracker tracker_host0.foo.com:localhost/127.0.0.1:33030 to host host0.foo.com
14/01/02 09:09:51 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
14/01/02 09:09:51 INFO mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1
14/01/02 09:09:51 INFO mapred.TaskTracker: Starting tasktracker with owner as pkeni
14/01/02 09:09:51 INFO mapred.TaskTracker: Good mapred local directories are: /tmp/hadoop-pkeni/mapred/local/1_0
14/01/02 09:09:51 INFO ipc.Server: Starting SocketReader
14/01/02 09:09:51 INFO ipc.Server: IPC Server Responder: starting
14/01/02 09:09:51 INFO ipc.Server: IPC Server listener on 54485: starting
14/01/02 09:09:51 INFO ipc.Server: IPC Server handler 0 on 54485: starting
14/01/02 09:09:51 INFO ipc.Server: IPC Server handler 1 on 54485: starting
14/01/02 09:09:51 INFO ipc.Server: IPC Server handler 3 on 54485: starting
14/01/02 09:09:51 INFO mapred.TaskTracker: TaskTracker up at: localhost/127.0.0.1:54485
14/01/02 09:09:51 INFO mapred.TaskTracker: Starting tracker tracker_host1.foo.com:localhost/127.0.0.1:54485
14/01/02 09:09:51 INFO ipc.Server: IPC Server handler 2 on 54485: starting
14/01/02 09:09:51 INFO mapred.TaskTracker: Starting thread: Map-events fetcher for all reduce tasks on tracker_host1.foo.com:localhost/127.0.0.1:54485
14/01/02 09:09:51 INFO mapred.TaskTracker:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@2e4a0035
14/01/02 09:09:51 WARN mapred.TaskTracker: TaskTracker's totalMemoryAllottedForTasks is -1. TaskMemoryManager is disabled.
14/01/02 09:09:51 INFO mapred.IndexCache: IndexCache created with max memory = 10485760
14/01/02 09:09:51 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 0
14/01/02 09:09:51 INFO http.HttpServer: listener.getLocalPort() returned 49386 webServer.getConnectors()[0].getLocalPort() returned 49386
14/01/02 09:09:51 INFO http.HttpServer: Jetty bound to port 49386
2014-01-02 09:09:51.624:INFO::jetty-6.1.26
2014-01-02 09:09:51.628:INFO::Extract jar:file:/home/pkeni/.m2/repository/org/apache/hadoop/hadoop-core/1.1.1/hadoop-core-1.1.1.jar!/webapps/task to /tmp/Jetty_0_0_0_0_49386_task____ub7p4a/webapp
2014-01-02 09:09:51.738:INFO::Started SelectChannelConnector@0.0.0.0:49386
14/01/02 09:09:51 INFO mapred.TaskTracker: FILE_CACHE_SIZE for mapOutputServlet set to : 2000
14/01/02 09:09:51 WARN fs.FileSystem: "localhost:34622" is a deprecated filesystem name. Use "hdfs://localhost:34622/" instead.
14/01/02 09:09:51 INFO mapred.UserLogCleaner: Adding job_20140102083937430_0001 for user-log deletion with retainTimeStamp:1388768991622
14/01/02 09:09:51 INFO mapred.UserLogCleaner: Adding job_20140102083937430_0001 for user-log deletion with retainTimeStamp:1388768991622
14/01/02 09:09:52 INFO net.NetworkTopology: Adding a new node: /default-rack/host1.foo.com
14/01/02 09:09:52 INFO mapred.JobTracker: Adding tracker tracker_host1.foo.com:localhost/127.0.0.1:54485 to host host1.foo.com
14/01/02 09:09:52 WARN fs.FileSystem: "localhost:34622" is a deprecated filesystem name. Use "hdfs://localhost:34622/" instead.
14/01/02 09:09:52 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/pkeni/input/sample.txt. blk_1861426802774877726_1002
14/01/02 09:09:52 INFO datanode.DataNode: Receiving block blk_1861426802774877726_1002 src: /127.0.0.1:56801 dest: /127.0.0.1:45383
14/01/02 09:09:52 INFO datanode.DataNode: Receiving block blk_1861426802774877726_1002 src: /127.0.0.1:49925 dest: /127.0.0.1:55965
14/01/02 09:09:52 INFO DataNode.clienttrace: src: /127.0.0.1:49925, dest: /127.0.0.1:55965, bytes: 529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_1861426802774877726_1002, duration: 5179795
14/01/02 09:09:52 INFO datanode.DataNode: PacketResponder 0 for block blk_1861426802774877726_1002 terminating
14/01/02 09:09:52 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_1861426802774877726_1002 size 529
14/01/02 09:09:52 INFO DataNode.clienttrace: src: /127.0.0.1:56801, dest: /127.0.0.1:45383, bytes: 529, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_1861426802774877726_1002, duration: 6163234
14/01/02 09:09:52 INFO datanode.DataNode: PacketResponder 1 for block blk_1861426802774877726_1002 terminating
14/01/02 09:09:52 INFO hdfs.StateChange: Removing lease on  file /user/pkeni/input/sample.txt from client DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:52 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /user/pkeni/input/sample.txt is closed by DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:52 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_1861426802774877726_1002 size 529
14/01/02 09:09:52 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/01/02 09:09:53 WARN mapred.JobClient: No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).
14/01/02 09:09:53 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:09:53 INFO namenode.FSNamesystem: Increasing replication for file /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.split. New replication is 10
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.split. blk_-6819589597827219293_1003
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_-6819589597827219293_1003 src: /127.0.0.1:49926 dest: /127.0.0.1:55965
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_-6819589597827219293_1003 src: /127.0.0.1:56804 dest: /127.0.0.1:45383
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:56804, dest: /127.0.0.1:45383, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-6819589597827219293_1003, duration: 910639
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 0 for block blk_-6819589597827219293_1003 terminating
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:49926, dest: /127.0.0.1:55965, bytes: 122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_-6819589597827219293_1003, duration: 1493521
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 1 for block blk_-6819589597827219293_1003 terminating
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_-6819589597827219293_1003 size 122
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_-6819589597827219293_1003 size 122
14/01/02 09:09:53 INFO hdfs.StateChange: Removing lease on  file /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.split from client DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.split is closed by DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.splitmetainfo. blk_1702179668490923704_1004
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_1702179668490923704_1004 src: /127.0.0.1:49928 dest: /127.0.0.1:55965
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_1702179668490923704_1004 src: /127.0.0.1:56806 dest: /127.0.0.1:45383
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:56806, dest: /127.0.0.1:45383, bytes: 35, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_1702179668490923704_1004, duration: 400632
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 0 for block blk_1702179668490923704_1004 terminating
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:49928, dest: /127.0.0.1:55965, bytes: 35, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_1702179668490923704_1004, duration: 1147173
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 1 for block blk_1702179668490923704_1004 terminating
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_1702179668490923704_1004 size 35
14/01/02 09:09:53 INFO hdfs.StateChange: Removing lease on  file /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.splitmetainfo from client DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_1702179668490923704_1004 size 35
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.xml. blk_7183848477329415750_1005
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_7183848477329415750_1005 src: /127.0.0.1:49930 dest: /127.0.0.1:55965
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_7183848477329415750_1005 src: /127.0.0.1:56808 dest: /127.0.0.1:45383
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:56808, dest: /127.0.0.1:45383, bytes: 22297, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_7183848477329415750_1005, duration: 2071258
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 0 for block blk_7183848477329415750_1005 terminating
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_7183848477329415750_1005 size 22297
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:49930, dest: /127.0.0.1:55965, bytes: 22297, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_7183848477329415750_1005, duration: 4531986
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 1 for block blk_7183848477329415750_1005 terminating
14/01/02 09:09:53 INFO hdfs.StateChange: Removing lease on  file /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.xml from client DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /tmp/hadoop-pkeni/mapred/staging/pkeni/.staging/job_20140102090939353_0001/job.xml is closed by DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_7183848477329415750_1005 size 22297
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:45383, dest: /127.0.0.1:56810, bytes: 22473, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_723611057_103, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_7183848477329415750_1005, duration: 2150143
14/01/02 09:09:53 INFO mapred.JobInProgress: job_20140102090939353_0001: nMaps=1 nReduces=1 max=-1
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /tmp/hadoop-pkeni/mapred/system/job_20140102090939353_0001/job-info. blk_-1368905318166974339_1006
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_-1368905318166974339_1006 src: /127.0.0.1:56811 dest: /127.0.0.1:45383
14/01/02 09:09:53 INFO datanode.DataNode: Receiving block blk_-1368905318166974339_1006 src: /127.0.0.1:49935 dest: /127.0.0.1:55965
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:49935, dest: /127.0.0.1:55965, bytes: 128, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_-1368905318166974339_1006, duration: 385244
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 0 for block blk_-1368905318166974339_1006 terminating
14/01/02 09:09:53 INFO DataNode.clienttrace: src: /127.0.0.1:56811, dest: /127.0.0.1:45383, bytes: 128, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-1368905318166974339_1006, duration: 1287345
14/01/02 09:09:53 INFO datanode.DataNode: PacketResponder 1 for block blk_-1368905318166974339_1006 terminating
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_-1368905318166974339_1006 size 128
14/01/02 09:09:53 INFO hdfs.StateChange: Removing lease on  file /tmp/hadoop-pkeni/mapred/system/job_20140102090939353_0001/job-info from client DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /tmp/hadoop-pkeni/mapred/system/job_20140102090939353_0001/job-info is closed by DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:53 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_-1368905318166974339_1006 size 128
14/01/02 09:09:53 INFO mapred.JobTracker: Job job_20140102090939353_0001 added successfully for user 'pkeni' to queue 'default'
14/01/02 09:09:53 INFO mapred.JobTracker: Initializing job_20140102090939353_0001
14/01/02 09:09:53 INFO mapred.JobInProgress: Initializing job_20140102090939353_0001
14/01/02 09:09:53 INFO mapred.AuditLogger: USER=pkeni	IP=127.0.0.1	OPERATION=SUBMIT_JOB	TARGET=job_20140102090939353_0001	RESULT=SUCCESS
14/01/02 09:09:53 INFO mapred.JobClient: Running job: job_20140102090939353_0001
14/01/02 09:09:54 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/pkeni/output/_logs/history/job_20140102090939353_0001_conf.xml. blk_-950902840431405365_1008
14/01/02 09:09:54 INFO datanode.DataNode: Receiving block blk_-950902840431405365_1008 src: /127.0.0.1:56815 dest: /127.0.0.1:45383
14/01/02 09:09:54 INFO datanode.DataNode: Receiving block blk_-950902840431405365_1008 src: /127.0.0.1:49939 dest: /127.0.0.1:55965
14/01/02 09:09:54 INFO DataNode.clienttrace: src: /127.0.0.1:49939, dest: /127.0.0.1:55965, bytes: 22361, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_723611057_103, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_-950902840431405365_1008, duration: 9838959
14/01/02 09:09:54 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_-950902840431405365_1008 size 22361
14/01/02 09:09:54 INFO datanode.DataNode: PacketResponder 0 for block blk_-950902840431405365_1008 terminating
14/01/02 09:09:54 INFO DataNode.clienttrace: src: /127.0.0.1:56815, dest: /127.0.0.1:45383, bytes: 22361, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_723611057_103, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-950902840431405365_1008, duration: 6846007
14/01/02 09:09:54 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_-950902840431405365_1008 size 22361
14/01/02 09:09:54 INFO datanode.DataNode: PacketResponder 1 for block blk_-950902840431405365_1008 terminating
14/01/02 09:09:54 INFO hdfs.StateChange: Removing lease on  file /user/pkeni/output/_logs/history/job_20140102090939353_0001_conf.xml from client DFSClient_NONMAPREDUCE_723611057_103
14/01/02 09:09:54 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /user/pkeni/output/_logs/history/job_20140102090939353_0001_conf.xml is closed by DFSClient_NONMAPREDUCE_723611057_103
14/01/02 09:09:54 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /tmp/hadoop-pkeni/mapred/system/job_20140102090939353_0001/jobToken. blk_-3675035483111029086_1009
14/01/02 09:09:54 INFO datanode.DataNode: Receiving block blk_-3675035483111029086_1009 src: /127.0.0.1:49940 dest: /127.0.0.1:55965
14/01/02 09:09:54 INFO datanode.DataNode: Receiving block blk_-3675035483111029086_1009 src: /127.0.0.1:56818 dest: /127.0.0.1:45383
14/01/02 09:09:54 INFO DataNode.clienttrace: src: /127.0.0.1:56818, dest: /127.0.0.1:45383, bytes: 116, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-3675035483111029086_1009, duration: 445954
14/01/02 09:09:54 INFO datanode.DataNode: PacketResponder 0 for block blk_-3675035483111029086_1009 terminating
14/01/02 09:09:54 INFO DataNode.clienttrace: src: /127.0.0.1:49940, dest: /127.0.0.1:55965, bytes: 116, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_-3675035483111029086_1009, duration: 1376771
14/01/02 09:09:54 INFO datanode.DataNode: PacketResponder 1 for block blk_-3675035483111029086_1009 terminating
14/01/02 09:09:54 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_-3675035483111029086_1009 size 116
14/01/02 09:09:54 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_-3675035483111029086_1009 size 116
14/01/02 09:09:54 INFO hdfs.StateChange: Removing lease on  file /tmp/hadoop-pkeni/mapred/system/job_20140102090939353_0001/jobToken from client DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:54 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /tmp/hadoop-pkeni/mapred/system/job_20140102090939353_0001/jobToken is closed by DFSClient_NONMAPREDUCE_-445732831_1
14/01/02 09:09:54 INFO mapred.JobInProgress: jobToken generated and stored with users keys in /tmp/hadoop-pkeni/mapred/system/job_20140102090939353_0001/jobToken
14/01/02 09:09:54 INFO DataNode.clienttrace: src: /127.0.0.1:55965, dest: /127.0.0.1:49942, bytes: 39, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_723611057_103, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_1702179668490923704_1004, duration: 481452
14/01/02 09:09:54 INFO mapred.JobInProgress: Input size for job job_20140102090939353_0001 = 529. Number of splits = 1
14/01/02 09:09:54 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1
14/01/02 09:09:54 INFO mapred.JobInProgress: tip:task_20140102090939353_0001_m_000000 has split on node:/default-rack/127.0.0.1
14/01/02 09:09:54 INFO mapred.JobInProgress: tip:task_20140102090939353_0001_m_000000 has split on node:/default-rack/127.0.0.1
14/01/02 09:09:54 INFO mapred.JobInProgress: job_20140102090939353_0001 LOCALITY_WAIT_FACTOR=0.5
14/01/02 09:09:54 INFO mapred.JobInProgress: Job job_20140102090939353_0001 initialized successfully with 1 map tasks and 1 reduce tasks.
14/01/02 09:09:54 INFO mapred.JobTracker: Adding task (JOB_SETUP) 'attempt_20140102090939353_0001_m_000002_0' to tip task_20140102090939353_0001_m_000002, for tracker 'tracker_host0.foo.com:localhost/127.0.0.1:33030'
14/01/02 09:09:54 INFO mapred.TaskTracker: LaunchTaskAction (registerTask): attempt_20140102090939353_0001_m_000002_0 task's state:UNASSIGNED
14/01/02 09:09:54 INFO mapred.TaskTracker: Trying to launch : attempt_20140102090939353_0001_m_000002_0 which needs 1 slots
14/01/02 09:09:54 INFO mapred.TaskTracker: In TaskLauncher, current free slots : 2 and trying to launch attempt_20140102090939353_0001_m_000002_0 which needs 1 slots
14/01/02 09:09:54 INFO DataNode.clienttrace: src: /127.0.0.1:45383, dest: /127.0.0.1:56820, bytes: 120, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-3675035483111029086_1009, duration: 307720
14/01/02 09:09:54 INFO DataNode.clienttrace: src: /127.0.0.1:45383, dest: /127.0.0.1:56822, bytes: 22473, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-1961550873_213, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_7183848477329415750_1005, duration: 277495
14/01/02 09:09:54 INFO mapred.JobLocalizer: Initializing user pkeni on this TT.
14/01/02 09:09:54 INFO mapred.JobClient:  map 0% reduce 0%
14/01/02 09:09:55 INFO mapred.JvmManager: In JvmRunner constructed JVM ID: jvm_20140102090939353_0001_m_2032974864
14/01/02 09:09:55 INFO mapred.JvmManager: JVM Runner jvm_20140102090939353_0001_m_2032974864 spawned.
14/01/02 09:09:55 INFO mapred.TaskController: Writing commands to /tmp/hadoop-pkeni/mapred/local/0_0/ttprivate/taskTracker/pkeni/jobcache/job_20140102090939353_0001/attempt_20140102090939353_0001_m_000002_0/taskjvm.sh
14/01/02 09:09:55 INFO mapred.TaskTracker: JVM with ID: jvm_20140102090939353_0001_m_2032974864 given task: attempt_20140102090939353_0001_m_000002_0
14/01/02 09:09:56 INFO mapred.TaskTracker: attempt_20140102090939353_0001_m_000002_0 0.0% setup
14/01/02 09:09:56 INFO mapred.TaskTracker: Task attempt_20140102090939353_0001_m_000002_0 is done.
14/01/02 09:09:56 INFO mapred.TaskTracker: reported output size for attempt_20140102090939353_0001_m_000002_0  was -1
14/01/02 09:09:56 INFO mapred.TaskTracker: addFreeSlot : current free slots : 2
14/01/02 09:09:56 INFO mapred.JvmManager: JVM : jvm_20140102090939353_0001_m_2032974864 exited with exit code 0. Number of tasks it ran: 1
14/01/02 09:09:56 INFO mapred.JobInProgress: Task 'attempt_20140102090939353_0001_m_000002_0' has completed task_20140102090939353_0001_m_000002 successfully.
14/01/02 09:09:56 INFO mapred.JobTracker: Adding task (MAP) 'attempt_20140102090939353_0001_m_000000_0' to tip task_20140102090939353_0001_m_000000, for tracker 'tracker_host0.foo.com:localhost/127.0.0.1:33030'
14/01/02 09:09:56 INFO mapred.JobInProgress: Choosing rack-local task task_20140102090939353_0001_m_000000
14/01/02 09:09:56 INFO mapred.TaskTracker: LaunchTaskAction (registerTask): attempt_20140102090939353_0001_m_000000_0 task's state:UNASSIGNED
14/01/02 09:09:56 INFO mapred.TaskTracker: Trying to launch : attempt_20140102090939353_0001_m_000000_0 which needs 1 slots
14/01/02 09:09:56 INFO mapred.TaskTracker: In TaskLauncher, current free slots : 2 and trying to launch attempt_20140102090939353_0001_m_000000_0 which needs 1 slots
14/01/02 09:09:56 INFO mapred.JvmManager: In JvmRunner constructed JVM ID: jvm_20140102090939353_0001_m_1731447513
14/01/02 09:09:56 INFO mapred.JvmManager: JVM Runner jvm_20140102090939353_0001_m_1731447513 spawned.
14/01/02 09:09:56 INFO mapred.TaskController: Writing commands to /tmp/hadoop-pkeni/mapred/local/0_0/ttprivate/taskTracker/pkeni/jobcache/job_20140102090939353_0001/attempt_20140102090939353_0001_m_000000_0/taskjvm.sh
14/01/02 09:09:56 INFO mapred.JobClient: Task Id : attempt_20140102090939353_0001_m_000002_0, Status : SUCCEEDED
14/01/02 09:09:57 INFO mapred.TaskTracker: JVM with ID: jvm_20140102090939353_0001_m_1731447513 given task: attempt_20140102090939353_0001_m_000000_0
14/01/02 09:09:58 INFO DataNode.clienttrace: src: /127.0.0.1:45383, dest: /127.0.0.1:56828, bytes: 126, op: HDFS_READ, cliID: DFSClient_attempt_20140102090939353_0001_m_000000_0_700695322_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-6819589597827219293_1003, duration: 292518
14/01/02 09:09:58 INFO DataNode.clienttrace: src: /127.0.0.1:45383, dest: /127.0.0.1:56829, bytes: 537, op: HDFS_READ, cliID: DFSClient_attempt_20140102090939353_0001_m_000000_0_700695322_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_1861426802774877726_1002, duration: 423035
14/01/02 09:09:58 INFO mapred.TaskTracker: attempt_20140102090939353_0001_m_000000_0 1.0% 
14/01/02 09:09:58 INFO mapred.TaskTracker: Task attempt_20140102090939353_0001_m_000000_0 is done.
14/01/02 09:09:58 INFO mapred.TaskTracker: reported output size for attempt_20140102090939353_0001_m_000000_0  was 28
14/01/02 09:09:58 INFO mapred.TaskTracker: addFreeSlot : current free slots : 2
14/01/02 09:09:58 INFO mapred.JvmManager: JVM : jvm_20140102090939353_0001_m_1731447513 exited with exit code 0. Number of tasks it ran: 1
14/01/02 09:09:58 INFO mapred.JobInProgress: Task 'attempt_20140102090939353_0001_m_000000_0' has completed task_20140102090939353_0001_m_000000 successfully.
14/01/02 09:09:58 INFO mapred.JobTracker: Adding task (REDUCE) 'attempt_20140102090939353_0001_r_000000_0' to tip task_20140102090939353_0001_r_000000, for tracker 'tracker_host0.foo.com:localhost/127.0.0.1:33030'
14/01/02 09:09:58 INFO mapred.TaskTracker: LaunchTaskAction (registerTask): attempt_20140102090939353_0001_r_000000_0 task's state:UNASSIGNED
14/01/02 09:09:58 INFO mapred.TaskTracker: Trying to launch : attempt_20140102090939353_0001_r_000000_0 which needs 1 slots
14/01/02 09:09:58 INFO mapred.TaskTracker: In TaskLauncher, current free slots : 2 and trying to launch attempt_20140102090939353_0001_r_000000_0 which needs 1 slots
14/01/02 09:09:58 INFO mapred.JvmManager: In JvmRunner constructed JVM ID: jvm_20140102090939353_0001_r_2032974864
14/01/02 09:09:58 INFO mapred.JvmManager: JVM Runner jvm_20140102090939353_0001_r_2032974864 spawned.
14/01/02 09:09:58 INFO mapred.TaskTracker: Received KillTaskAction for task: attempt_20140102090939353_0001_m_000002_0
14/01/02 09:09:58 INFO mapred.TaskTracker: About to purge task: attempt_20140102090939353_0001_m_000002_0
14/01/02 09:09:58 INFO mapred.IndexCache: Map ID attempt_20140102090939353_0001_m_000002_0 not found in cache
14/01/02 09:09:58 INFO mapred.TaskController: Writing commands to /tmp/hadoop-pkeni/mapred/local/0_0/ttprivate/taskTracker/pkeni/jobcache/job_20140102090939353_0001/attempt_20140102090939353_0001_r_000000_0/taskjvm.sh
14/01/02 09:09:59 INFO mapred.JobClient: Task Id : attempt_20140102090939353_0001_m_000000_0, Status : SUCCEEDED
14/01/02 09:09:59 INFO mapred.TaskTracker: JVM with ID: jvm_20140102090939353_0001_r_2032974864 given task: attempt_20140102090939353_0001_r_000000_0
14/01/02 09:10:00 INFO mapred.JobClient:  map 100% reduce 0%
14/01/02 09:10:05 INFO TaskTracker.clienttrace: src: 127.0.0.1:35016, dest: 127.0.0.1:37017, bytes: 28, op: MAPRED_SHUFFLE, cliID: attempt_20140102090939353_0001_m_000000_0, duration: 5256255
14/01/02 09:10:05 INFO mapred.TaskTracker: attempt_20140102090939353_0001_r_000000_0 0.33333334% reduce > copy (1 of 1 at 0.00 MB/s)
14/01/02 09:10:05 INFO mapred.TaskTracker: attempt_20140102090939353_0001_r_000000_0 0.33333334% reduce > copy (1 of 1 at 0.00 MB/s)
14/01/02 09:10:06 INFO mapred.TaskTracker: attempt_20140102090939353_0001_r_000000_0 0.33333334% reduce > copy (1 of 1 at 0.00 MB/s)
14/01/02 09:10:06 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/pkeni/output/_temporary/_attempt_20140102090939353_0001_r_000000_0/part-r-00000. blk_-5274054177946084189_1010
14/01/02 09:10:06 INFO datanode.DataNode: Receiving block blk_-5274054177946084189_1010 src: /127.0.0.1:49956 dest: /127.0.0.1:55965
14/01/02 09:10:06 INFO datanode.DataNode: Receiving block blk_-5274054177946084189_1010 src: /127.0.0.1:56834 dest: /127.0.0.1:45383
14/01/02 09:10:06 INFO DataNode.clienttrace: src: /127.0.0.1:56834, dest: /127.0.0.1:45383, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_attempt_20140102090939353_0001_r_000000_0_752760954_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-5274054177946084189_1010, duration: 3233100
14/01/02 09:10:06 INFO datanode.DataNode: PacketResponder 0 for block blk_-5274054177946084189_1010 terminating
14/01/02 09:10:06 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_-5274054177946084189_1010 size 17
14/01/02 09:10:06 INFO DataNode.clienttrace: src: /127.0.0.1:49956, dest: /127.0.0.1:55965, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_attempt_20140102090939353_0001_r_000000_0_752760954_1, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_-5274054177946084189_1010, duration: 10167729
14/01/02 09:10:06 INFO datanode.DataNode: PacketResponder 1 for block blk_-5274054177946084189_1010 terminating
14/01/02 09:10:06 INFO hdfs.StateChange: Removing lease on  file /user/pkeni/output/_temporary/_attempt_20140102090939353_0001_r_000000_0/part-r-00000 from client DFSClient_attempt_20140102090939353_0001_r_000000_0_752760954_1
14/01/02 09:10:06 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /user/pkeni/output/_temporary/_attempt_20140102090939353_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_20140102090939353_0001_r_000000_0_752760954_1
14/01/02 09:10:06 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_-5274054177946084189_1010 size 17
14/01/02 09:10:06 INFO mapred.TaskTracker: Task attempt_20140102090939353_0001_r_000000_0 is in commit-pending, task state:COMMIT_PENDING
14/01/02 09:10:06 INFO mapred.TaskTracker: attempt_20140102090939353_0001_r_000000_0 0.33333334% reduce > copy (1 of 1 at 0.00 MB/s)
14/01/02 09:10:06 INFO mapred.TaskTracker: Received commit task action for attempt_20140102090939353_0001_r_000000_0
14/01/02 09:10:07 INFO mapred.JobClient:  map 100% reduce 33%
14/01/02 09:10:07 INFO mapred.TaskTracker: attempt_20140102090939353_0001_r_000000_0 1.0% reduce > reduce
14/01/02 09:10:07 INFO mapred.TaskTracker: Task attempt_20140102090939353_0001_r_000000_0 is done.
14/01/02 09:10:07 INFO mapred.TaskTracker: reported output size for attempt_20140102090939353_0001_r_000000_0  was -1
14/01/02 09:10:07 INFO mapred.TaskTracker: addFreeSlot : current free slots : 2
14/01/02 09:10:07 INFO mapred.JobInProgress: Task 'attempt_20140102090939353_0001_r_000000_0' has completed task_20140102090939353_0001_r_000000 successfully.
14/01/02 09:10:07 INFO mapred.JobTracker: Adding task (JOB_CLEANUP) 'attempt_20140102090939353_0001_m_000001_0' to tip task_20140102090939353_0001_m_000001, for tracker 'tracker_host0.foo.com:localhost/127.0.0.1:33030'
14/01/02 09:10:07 INFO mapred.TaskTracker: LaunchTaskAction (registerTask): attempt_20140102090939353_0001_m_000001_0 task's state:UNASSIGNED
14/01/02 09:10:07 INFO mapred.TaskTracker: Trying to launch : attempt_20140102090939353_0001_m_000001_0 which needs 1 slots
14/01/02 09:10:07 INFO mapred.TaskTracker: In TaskLauncher, current free slots : 2 and trying to launch attempt_20140102090939353_0001_m_000001_0 which needs 1 slots
14/01/02 09:10:07 INFO mapred.JvmManager: In JvmRunner constructed JVM ID: jvm_20140102090939353_0001_m_-1358153794
14/01/02 09:10:07 INFO mapred.JvmManager: JVM Runner jvm_20140102090939353_0001_m_-1358153794 spawned.
14/01/02 09:10:07 INFO mapred.TaskController: Writing commands to /tmp/hadoop-pkeni/mapred/local/0_0/ttprivate/taskTracker/pkeni/jobcache/job_20140102090939353_0001/attempt_20140102090939353_0001_m_000001_0/taskjvm.sh
14/01/02 09:10:07 INFO mapred.JvmManager: JVM : jvm_20140102090939353_0001_r_2032974864 exited with exit code 0. Number of tasks it ran: 1
14/01/02 09:10:08 INFO mapred.TaskTracker: JVM with ID: jvm_20140102090939353_0001_m_-1358153794 given task: attempt_20140102090939353_0001_m_000001_0
14/01/02 09:10:08 INFO mapred.JobClient: Task Id : attempt_20140102090939353_0001_r_000000_0, Status : SUCCEEDED
14/01/02 09:10:08 INFO mapred.TaskTracker: Received KillTaskAction for task: attempt_20140102090939353_0001_r_000000_0
14/01/02 09:10:08 INFO mapred.TaskTracker: About to purge task: attempt_20140102090939353_0001_r_000000_0
14/01/02 09:10:08 INFO mapred.TaskTracker: attempt_20140102090939353_0001_m_000001_0 0.0% 
14/01/02 09:10:08 INFO hdfs.StateChange: Removing lease on  file /user/pkeni/output/_SUCCESS from client DFSClient_attempt_20140102090939353_0001_m_000001_0_-1642579341_1
14/01/02 09:10:08 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /user/pkeni/output/_SUCCESS is closed by DFSClient_attempt_20140102090939353_0001_m_000001_0_-1642579341_1
14/01/02 09:10:08 INFO hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_-6819589597827219293 to 127.0.0.1:45383 127.0.0.1:55965 
14/01/02 09:10:08 INFO hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_1702179668490923704 to 127.0.0.1:55965 127.0.0.1:45383 
14/01/02 09:10:08 INFO hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_7183848477329415750 to 127.0.0.1:45383 127.0.0.1:55965 
14/01/02 09:10:09 INFO mapred.TaskTracker: attempt_20140102090939353_0001_m_000001_0 0.0% cleanup
14/01/02 09:10:09 INFO mapred.TaskTracker: Task attempt_20140102090939353_0001_m_000001_0 is done.
14/01/02 09:10:09 INFO mapred.TaskTracker: reported output size for attempt_20140102090939353_0001_m_000001_0  was -1
14/01/02 09:10:09 INFO mapred.TaskTracker: addFreeSlot : current free slots : 2
14/01/02 09:10:09 INFO mapred.JvmManager: JVM : jvm_20140102090939353_0001_m_-1358153794 exited with exit code 0. Number of tasks it ran: 1
14/01/02 09:10:09 INFO mapred.JobInProgress: Task 'attempt_20140102090939353_0001_m_000001_0' has completed task_20140102090939353_0001_m_000001 successfully.
14/01/02 09:10:09 INFO mapred.JobInProgress: Job job_20140102090939353_0001 has completed successfully.
14/01/02 09:10:09 INFO mapred.JobInProgress$JobSummary: jobId=job_20140102090939353_0001,submitTime=1388682593806,launchTime=1388682594652,firstMapTaskLaunchTime=1388682596743,firstReduceTaskLaunchTime=1388682598575,firstJobSetupTaskLaunchTime=1388682594901,firstJobCleanupTaskLaunchTime=1388682607388,finishTime=1388682609203,numMaps=1,numSlotsPerMap=1,numReduces=1,numSlotsPerReduce=1,user=pkeni,queue=default,status=SUCCEEDED,mapSlotSeconds=4,reduceSlotsSeconds=8,clusterMapCapacity=4,clusterReduceCapacity=4,jobName=Max temperature
14/01/02 09:10:09 INFO hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/pkeni/output/_logs/history/job_20140102090939353_0001_1388682593806_pkeni_Max+temperature. blk_-1539307912377135383_1011
14/01/02 09:10:09 INFO datanode.DataNode: Receiving block blk_-1539307912377135383_1011 src: /127.0.0.1:56839 dest: /127.0.0.1:45383
14/01/02 09:10:09 INFO datanode.DataNode: Receiving block blk_-1539307912377135383_1011 src: /127.0.0.1:49963 dest: /127.0.0.1:55965
14/01/02 09:10:09 INFO DataNode.clienttrace: src: /127.0.0.1:49963, dest: /127.0.0.1:55965, bytes: 13625, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_723611057_103, offset: 0, srvID: DS-1988144180-127.0.1.1-55965-1388682578550, blockid: blk_-1539307912377135383_1011, duration: 1205732
14/01/02 09:10:09 INFO datanode.DataNode: PacketResponder 0 for block blk_-1539307912377135383_1011 terminating
14/01/02 09:10:09 INFO DataNode.clienttrace: src: /127.0.0.1:56839, dest: /127.0.0.1:45383, bytes: 13625, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_723611057_103, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-1539307912377135383_1011, duration: 1970650
14/01/02 09:10:09 INFO datanode.DataNode: PacketResponder 1 for block blk_-1539307912377135383_1011 terminating
14/01/02 09:10:09 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:45383 is added to blk_-1539307912377135383_1011 size 13625
14/01/02 09:10:09 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:55965 is added to blk_-1539307912377135383_1011 size 13625
14/01/02 09:10:09 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:10:09 INFO hdfs.StateChange: Removing lease on  file /user/pkeni/output/_logs/history/job_20140102090939353_0001_1388682593806_pkeni_Max+temperature from client DFSClient_NONMAPREDUCE_723611057_103
14/01/02 09:10:09 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /user/pkeni/output/_logs/history/job_20140102090939353_0001_1388682593806_pkeni_Max+temperature is closed by DFSClient_NONMAPREDUCE_723611057_103
14/01/02 09:10:09 INFO mapred.JobHistory: Creating DONE subfolder at file:/tmp/history/done/version-1/localhost_1388682579397_/2014/01/02/000000
14/01/02 09:10:09 INFO mapred.JobHistory: Moving file:/tmp/history/job_20140102090939353_0001_1388682593806_pkeni_Max+temperature to file:/tmp/history/done/version-1/localhost_1388682579397_/2014/01/02/000000
14/01/02 09:10:09 INFO mapred.JobTracker: Removing task 'attempt_20140102090939353_0001_m_000000_0'
14/01/02 09:10:09 INFO mapred.JobTracker: Removing task 'attempt_20140102090939353_0001_m_000001_0'
14/01/02 09:10:09 INFO mapred.JobTracker: Removing task 'attempt_20140102090939353_0001_m_000002_0'
14/01/02 09:10:09 INFO mapred.JobTracker: Removing task 'attempt_20140102090939353_0001_r_000000_0'
14/01/02 09:10:09 INFO mapred.JobClient: Task Id : attempt_20140102090939353_0001_m_000001_0, Status : SUCCEEDED
14/01/02 09:10:09 INFO mapred.JobHistory: Moving file:/tmp/history/job_20140102090939353_0001_conf.xml to file:/tmp/history/done/version-1/localhost_1388682579397_/2014/01/02/000000
14/01/02 09:10:09 INFO mapred.JobClient: Job complete: job_20140102090939353_0001
14/01/02 09:10:09 INFO mapred.JobClient: Counters: 29
14/01/02 09:10:09 INFO mapred.JobClient:   Job Counters 
14/01/02 09:10:09 INFO mapred.JobClient:     Launched reduce tasks=1
14/01/02 09:10:09 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=4604
14/01/02 09:10:09 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/01/02 09:10:09 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/01/02 09:10:09 INFO mapred.JobClient:     Rack-local map tasks=1
14/01/02 09:10:09 INFO mapred.JobClient:     Launched map tasks=1
14/01/02 09:10:09 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=8740
14/01/02 09:10:09 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:10:09 INFO mapred.JobClient:     Bytes Written=17
14/01/02 09:10:09 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:10:09 INFO mapred.JobClient:     FILE_BYTES_READ=28
14/01/02 09:10:09 INFO mapred.JobClient:     HDFS_BYTES_READ=644
14/01/02 09:10:09 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=47489
14/01/02 09:10:09 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=17
14/01/02 09:10:09 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:10:09 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:10:09 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:10:09 INFO mapred.JobClient:     Map output materialized bytes=28
14/01/02 09:10:09 INFO mapred.JobClient:     Map input records=5
14/01/02 09:10:09 INFO mapred.JobClient:     Reduce shuffle bytes=28
14/01/02 09:10:09 INFO mapred.JobClient:     Spilled Records=4
14/01/02 09:10:09 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:10:09 INFO mapred.JobClient:     Total committed heap usage (bytes)=402653184
14/01/02 09:10:09 INFO mapred.JobClient:     CPU time spent (ms)=1380
14/01/02 09:10:09 INFO mapred.JobClient:     Combine input records=5
14/01/02 09:10:09 INFO mapred.JobClient:     SPLIT_RAW_BYTES=115
14/01/02 09:10:09 INFO mapred.JobClient:     Reduce input records=2
14/01/02 09:10:09 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:10:09 INFO mapred.JobClient:     Combine output records=2
14/01/02 09:10:09 INFO mapred.JobClient:     Physical memory (bytes) snapshot=315740160
14/01/02 09:10:09 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:10:09 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=4007170048
14/01/02 09:10:09 INFO hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_-1368905318166974339 to 127.0.0.1:55965 127.0.0.1:45383 
14/01/02 09:10:09 INFO hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_-3675035483111029086 to 127.0.0.1:45383 127.0.0.1:55965 
14/01/02 09:10:09 INFO mapred.JobClient:     Map output records=5
14/01/02 09:10:09 INFO DataNode.clienttrace: src: /127.0.0.1:45383, dest: /127.0.0.1:56842, bytes: 21, op: HDFS_READ, cliID: DFSClient_NONMAPREDUCE_-445732831_1, offset: 0, srvID: DS-465270554-127.0.1.1-45383-1388682579294, blockid: blk_-5274054177946084189_1010, duration: 179741
14/01/02 09:10:09 INFO mapred.MiniMRCluster: Waiting for task tracker tracker_host0.foo.com:localhost/127.0.0.1:33030 to be idle.
14/01/02 09:10:09 INFO mapred.TaskTracker: Received 'KillJobAction' for job: job_20140102090939353_0001
14/01/02 09:10:09 WARN mapred.TaskTracker: Unknown job job_20140102090939353_0001 being deleted.
14/01/02 09:10:10 INFO hdfs.StateChange: BLOCK* ask 127.0.0.1:55965 to delete  blk_7183848477329415750_1005 blk_1702179668490923704_1004 blk_-6819589597827219293_1003 blk_-3675035483111029086_1009 blk_-1368905318166974339_1006
14/01/02 09:10:10 INFO mapred.TaskTracker: Received 'KillJobAction' for job: job_20140102090939353_0001
14/01/02 09:10:10 INFO mapred.IndexCache: Map ID attempt_20140102090939353_0001_m_000001_0 not found in cache
14/01/02 09:10:10 INFO mapred.UserLogCleaner: Adding job_20140102090939353_0001 for user-log deletion with retainTimeStamp:1388769010588
14/01/02 09:10:10 INFO mapred.TaskTracker: Shutting down: Map-events fetcher for all reduce tasks on tracker_host0.foo.com:localhost/127.0.0.1:33030
14/01/02 09:10:10 INFO filecache.TrackerDistributedCacheManager: Cleanup...
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.filecache.TrackerDistributedCacheManager$CleanupThread.run(TrackerDistributedCacheManager.java:958)
14/01/02 09:10:10 INFO ipc.Server: Stopping server on 33030
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 0 on 33030: exiting
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 1 on 33030: exiting
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 2 on 33030: exiting
14/01/02 09:10:10 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 3 on 33030: exiting
14/01/02 09:10:10 INFO ipc.Server: Stopping IPC Server listener on 33030
14/01/02 09:10:10 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:10 INFO mapred.TaskTracker: Shutting down StatusHttpServer
2014-01-02 09:10:10.717:INFO::Stopped SelectChannelConnector@0.0.0.0:0
14/01/02 09:10:10 INFO mapred.TaskTracker: Interrupted. Closing down.
14/01/02 09:10:10 INFO mapred.TaskTracker: Shutting down: Map-events fetcher for all reduce tasks on tracker_host1.foo.com:localhost/127.0.0.1:54485
14/01/02 09:10:10 INFO filecache.TrackerDistributedCacheManager: Cleanup...
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.filecache.TrackerDistributedCacheManager$CleanupThread.run(TrackerDistributedCacheManager.java:958)
14/01/02 09:10:10 INFO ipc.Server: Stopping server on 54485
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 0 on 54485: exiting
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 3 on 54485: exiting
14/01/02 09:10:10 INFO ipc.Server: Stopping IPC Server listener on 54485
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 1 on 54485: exiting
14/01/02 09:10:10 INFO ipc.Server: IPC Server handler 2 on 54485: exiting
14/01/02 09:10:10 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:10 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:10:10 INFO mapred.TaskTracker: Shutting down StatusHttpServer
2014-01-02 09:10:10.831:INFO::Stopped SelectChannelConnector@0.0.0.0:0
14/01/02 09:10:10 ERROR mapred.TaskTracker: Caught exception: java.io.IOException: Call to localhost/127.0.0.1:46232 failed on local exception: java.nio.channels.ClosedChannelException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1144)
	at org.apache.hadoop.ipc.Client.call(Client.java:1112)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at org.apache.hadoop.mapred.$Proxy11.heartbeat(Unknown Source)
	at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1995)
	at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1789)
	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2641)
	at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:217)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureReadOpen(SocketChannelImpl.java:252)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:295)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:55)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:361)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:841)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:786)

14/01/02 09:10:10 INFO mapred.JobTracker: Stopping infoServer
2014-01-02 09:10:10.941:INFO::Stopped SelectChannelConnector@localhost:0
14/01/02 09:10:11 INFO mapred.JobTracker: Stopping interTrackerServer
14/01/02 09:10:11 INFO ipc.Server: Stopping server on 46232
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 1 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 3 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 5 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 6 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 9 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 4 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 2 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 0 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 7 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 8 on 46232: exiting
14/01/02 09:10:11 INFO ipc.Server: Stopping IPC Server listener on 46232
14/01/02 09:10:11 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:11 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:10:11 INFO mapred.JobTracker: Stopping expireTrackers
14/01/02 09:10:11 INFO mapred.JobTracker: Stopped interTrackerServer
14/01/02 09:10:11 INFO mapred.JobTracker: Stopping retirer
14/01/02 09:10:11 INFO mapred.EagerTaskInitializationListener: Stopping Job Init Manager thread
14/01/02 09:10:11 INFO mapred.EagerTaskInitializationListener: JobInitManagerThread interrupted.
14/01/02 09:10:11 INFO mapred.EagerTaskInitializationListener: Shutting down thread pool
14/01/02 09:10:11 INFO mapred.JobTracker: Stopping expireLaunchingTasks
14/01/02 09:10:11 INFO mapred.JobTracker: Stopping job history server
14/01/02 09:10:11 INFO mapred.JobTracker: stopped all jobtracker services
Shutting down the Mini HDFS Cluster
Shutting down DataNode 1
2014-01-02 09:10:11.049:INFO::Stopped SelectChannelConnector@localhost:0
14/01/02 09:10:11 INFO ipc.Server: Stopping server on 33978
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 0 on 33978: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 2 on 33978: exiting
14/01/02 09:10:11 INFO ipc.Server: IPC Server handler 1 on 33978: exiting
14/01/02 09:10:11 INFO ipc.Server: Stopping IPC Server listener on 33978
14/01/02 09:10:11 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:11 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:10:11 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 1
14/01/02 09:10:11 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:45383, storageID=DS-465270554-127.0.1.1-45383-1388682579294, infoPort=39393, ipcPort=33978):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:10:11 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:10:11 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:10:11 INFO datanode.DataNode: Scheduling block blk_-6819589597827219293_1003 file /tmp/dfs/data/data1/current/blk_-6819589597827219293 for deletion
14/01/02 09:10:11 INFO datanode.DataNode: Scheduling block blk_-3675035483111029086_1009 file /tmp/dfs/data/data2/current/blk_-3675035483111029086 for deletion
14/01/02 09:10:11 INFO datanode.DataNode: Scheduling block blk_-1368905318166974339_1006 file /tmp/dfs/data/data2/current/blk_-1368905318166974339 for deletion
14/01/02 09:10:11 INFO datanode.DataNode: Scheduling block blk_1702179668490923704_1004 file /tmp/dfs/data/data2/current/blk_1702179668490923704 for deletion
14/01/02 09:10:11 INFO datanode.DataNode: Scheduling block blk_7183848477329415750_1005 file /tmp/dfs/data/data1/current/blk_7183848477329415750 for deletion
14/01/02 09:10:11 INFO datanode.DataNode: Deleted block blk_-6819589597827219293_1003 at file /tmp/dfs/data/data1/current/blk_-6819589597827219293
14/01/02 09:10:11 INFO datanode.DataNode: Deleted block blk_7183848477329415750_1005 at file /tmp/dfs/data/data1/current/blk_7183848477329415750
14/01/02 09:10:11 INFO datanode.DataNode: Deleted block blk_-3675035483111029086_1009 at file /tmp/dfs/data/data2/current/blk_-3675035483111029086
14/01/02 09:10:11 INFO datanode.DataNode: Deleted block blk_-1368905318166974339_1006 at file /tmp/dfs/data/data2/current/blk_-1368905318166974339
14/01/02 09:10:11 INFO datanode.DataNode: Deleted block blk_1702179668490923704_1004 at file /tmp/dfs/data/data2/current/blk_1702179668490923704
14/01/02 09:10:12 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:10:12 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:45383, storageID=DS-465270554-127.0.1.1-45383-1388682579294, infoPort=39393, ipcPort=33978):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data3/current,/tmp/dfs/data/data4/current'}
14/01/02 09:10:12 INFO ipc.Server: Stopping server on 33978
14/01/02 09:10:12 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:12 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:10:12 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:10:12 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:10:12 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId780228353
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId780228353
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.stopCluster(ClusterMapReduceTestCase.java:152)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.tearDown(ClusterMapReduceTestCase.java:163)
	at junit.framework.TestCase.runBare(TestCase.java:140)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
Shutting down DataNode 0
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:10:12 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
2014-01-02 09:10:12.162:INFO::Stopped SelectChannelConnector@localhost:0
14/01/02 09:10:12 INFO ipc.Server: Stopping server on 59585
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 0 on 59585: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 1 on 59585: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 2 on 59585: exiting
14/01/02 09:10:12 INFO ipc.Server: Stopping IPC Server listener on 59585
14/01/02 09:10:12 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:12 INFO ipc.Server: Stopping IPC Server Responder
14/01/02 09:10:12 WARN datanode.DataNode: DatanodeRegistration(127.0.0.1:55965, storageID=DS-1988144180-127.0.1.1-55965-1388682578550, infoPort=42783, ipcPort=59585):DataXceiveServer:java.nio.channels.AsynchronousCloseException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:205)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:248)
	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
	at java.lang.Thread.run(Thread.java:744)

14/01/02 09:10:12 INFO datanode.DataNode: Exiting DataXceiveServer
14/01/02 09:10:12 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:10:12 INFO datanode.DataBlockScanner: Exiting DataBlockScanner thread.
14/01/02 09:10:12 INFO datanode.DataNode: DatanodeRegistration(127.0.0.1:55965, storageID=DS-1988144180-127.0.1.1-55965-1388682578550, infoPort=42783, ipcPort=59585):Finishing DataNode in: FSDataset{dirpath='/tmp/dfs/data/data1/current,/tmp/dfs/data/data2/current'}
14/01/02 09:10:12 WARN util.MBeans: Hadoop:service=DataNode,name=DataNodeInfo
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=DataNodeInfo
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.unRegisterMXBean(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:805)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1540)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:10:12 INFO ipc.Server: Stopping server on 59585
14/01/02 09:10:12 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:12 INFO datanode.DataNode: Waiting for threadgroup to exit, active threads is 0
14/01/02 09:10:12 INFO datanode.FSDatasetAsyncDiskService: Shutting down all async disk service threads...
14/01/02 09:10:12 INFO datanode.FSDatasetAsyncDiskService: All async disk service threads have been shut down.
14/01/02 09:10:12 WARN util.MBeans: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-2045247679
javax.management.InstanceNotFoundException: Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId-2045247679
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.hadoop.metrics2.util.MBeans.unregister(MBeans.java:71)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.shutdown(FSDataset.java:2066)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:867)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:570)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:554)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.stopCluster(ClusterMapReduceTestCase.java:152)
	at org.apache.hadoop.mapred.ClusterMapReduceTestCase.tearDown(ClusterMapReduceTestCase.java:163)
	at junit.framework.TestCase.runBare(TestCase.java:140)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)
	at org.apache.maven.surefire.Surefire.run(Surefire.java:180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)
	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)
14/01/02 09:10:12 WARN datanode.FSDatasetAsyncDiskService: AsyncDiskService has already shut down.
2014-01-02 09:10:12.277:INFO::Stopped SelectChannelConnector@localhost:0
14/01/02 09:10:12 INFO namenode.DecommissionManager: Interrupted Monitor
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
	at java.lang.Thread.run(Thread.java:744)
14/01/02 09:10:12 WARN namenode.FSNamesystem: ReplicationMonitor thread received InterruptedException.java.lang.InterruptedException: sleep interrupted
14/01/02 09:10:12 INFO namenode.FSNamesystem: Number of transactions: 73 Total time for transactions(ms): 2Number of transactions batched in Syncs: 4 Number of syncs: 42 SyncTimes(ms): 820 588 
14/01/02 09:10:12 INFO namenode.FSEditLog: closing edit log: position=8554, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:10:12 INFO namenode.FSEditLog: close success: truncate to 8554, editlog=/tmp/dfs/name1/current/edits
14/01/02 09:10:12 INFO namenode.FSEditLog: closing edit log: position=8554, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:10:12 INFO namenode.FSEditLog: close success: truncate to 8554, editlog=/tmp/dfs/name2/current/edits
14/01/02 09:10:12 INFO ipc.Server: Stopping server on 34622
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 0 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 3 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 1 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 2 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 4 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 5 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 6 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 7 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 8 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: IPC Server handler 9 on 34622: exiting
14/01/02 09:10:12 INFO ipc.Server: Stopping IPC Server listener on 34622
14/01/02 09:10:12 INFO metrics.RpcInstrumentation: shut down
14/01/02 09:10:12 INFO ipc.Server: Stopping IPC Server Responder
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.044 sec
Running SingleResourceConfigurationTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 sec
Running v2.MaxTemperatureMapperTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.039 sec
Running v3.MaxTemperatureMapperTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.041 sec

Results :

Tests run: 17, Failures: 0, Errors: 0, Skipped: 1

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch05 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch05/target/ch05-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch05 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch05/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch05 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch05/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch05 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch05/target/ch05-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch05/3.0/ch05-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch05/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch05/3.0/ch05-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 7: MapReduce Types and Formats 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch07 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch07/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch07 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch07 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch07/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch07 ---
[INFO] Compiling 21 source files to /home/pkeni/git/hadoop-book/ch07/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch07 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch07/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch07 ---
[INFO] Compiling 1 source file to /home/pkeni/git/hadoop-book/ch07/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch07 ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/ch07/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running TextInputFormatsTest
14/01/02 09:10:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/01/02 09:10:14 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:14 INFO mapred.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:15 INFO mapred.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:15 INFO mapred.FileInputFormat: Total input paths to process : 1
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.693 sec

Results :

Tests run: 3, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch07 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch07/target/ch07-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch07 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch07/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch07 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch07/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch07 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch07/target/ch07-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch07/3.0/ch07-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch07/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch07/3.0/ch07-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 8: MapReduce Features 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch08 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch08/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch08 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch08 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch08 ---
[INFO] Compiling 28 source files to /home/pkeni/git/hadoop-book/ch08/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch08 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch08/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch08 ---
[INFO] Compiling 1 source file to /home/pkeni/git/hadoop-book/ch08/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch08 ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/ch08/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running KeyFieldBasedComparatorTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.702 sec

Results :

Tests run: 3, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch08 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch08/target/ch08-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch08 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch08/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch08 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch08/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch08 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch08/target/ch08-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch08/3.0/ch08-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch08/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch08/3.0/ch08-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 11: Pig 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch11 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch11/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch11 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch11 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch11/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch11 ---
[INFO] Compiling 4 source files to /home/pkeni/git/hadoop-book/ch11/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch11 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch11/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch11 ---
[INFO] Compiling 2 source files to /home/pkeni/git/hadoop-book/ch11/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch11 ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/ch11/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.hadoopbook.pig.IsGoodQualityTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.225 sec
Running com.hadoopbook.pig.RangeTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec

Results :

Tests run: 9, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch11 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch11/target/ch11-3.0.jar
[INFO] 
[INFO] --- maven-assembly-plugin:2.2.1:single (make-assembly) @ ch11 ---
[INFO] Reading assembly descriptor: ../book/src/main/assembly/jar.xml
[INFO] com/hadoopbook/pig/Trim.class already added, skipping
[INFO] com/hadoopbook/pig/Range.class already added, skipping
[INFO] com/hadoopbook/pig/IsGoodQuality.class already added, skipping
[INFO] com/hadoopbook/pig/CutLoadFunc.class already added, skipping
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch11/target/../../pig-examples.jar
[INFO] com/hadoopbook/pig/Trim.class already added, skipping
[INFO] com/hadoopbook/pig/Range.class already added, skipping
[INFO] com/hadoopbook/pig/IsGoodQuality.class already added, skipping
[INFO] com/hadoopbook/pig/CutLoadFunc.class already added, skipping
[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.
Instead of attaching the assembly file: /home/pkeni/git/hadoop-book/ch11/target/../../pig-examples.jar, it will become the file for main project artifact.
NOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!
[WARNING] Replacing pre-existing project main-artifact file: /home/pkeni/git/hadoop-book/ch11/target/ch11-3.0.jar
with assembly file: /home/pkeni/git/hadoop-book/ch11/target/../../pig-examples.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch11 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch11/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch11 ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch11/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch11 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch11/target/../../pig-examples.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch11/3.0/ch11-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch11/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch11/3.0/ch11-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 12: Hive 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch12 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch12/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch12 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch12 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch12/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch12 ---
[INFO] Compiling 3 source files to /home/pkeni/git/hadoop-book/ch12/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch12 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch12/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch12 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch12 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch12 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch12/target/ch12-3.0.jar
[INFO] 
[INFO] --- maven-assembly-plugin:2.2.1:single (make-assembly) @ ch12 ---
[INFO] Reading assembly descriptor: ../book/src/main/assembly/jar.xml
[INFO] com/hadoopbook/hive/Mean$MeanDoubleUDAFEvaluator.class already added, skipping
[INFO] com/hadoopbook/hive/Mean$MeanDoubleUDAFEvaluator$PartialResult.class already added, skipping
[INFO] com/hadoopbook/hive/Maximum$MaximumIntUDAFEvaluator.class already added, skipping
[INFO] com/hadoopbook/hive/Mean.class already added, skipping
[INFO] com/hadoopbook/hive/Maximum.class already added, skipping
[INFO] com/hadoopbook/hive/Strip.class already added, skipping
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch12/target/../../hive-examples.jar
[INFO] com/hadoopbook/hive/Mean$MeanDoubleUDAFEvaluator.class already added, skipping
[INFO] com/hadoopbook/hive/Mean$MeanDoubleUDAFEvaluator$PartialResult.class already added, skipping
[INFO] com/hadoopbook/hive/Maximum$MaximumIntUDAFEvaluator.class already added, skipping
[INFO] com/hadoopbook/hive/Mean.class already added, skipping
[INFO] com/hadoopbook/hive/Maximum.class already added, skipping
[INFO] com/hadoopbook/hive/Strip.class already added, skipping
[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.
Instead of attaching the assembly file: /home/pkeni/git/hadoop-book/ch12/target/../../hive-examples.jar, it will become the file for main project artifact.
NOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!
[WARNING] Replacing pre-existing project main-artifact file: /home/pkeni/git/hadoop-book/ch12/target/ch12-3.0.jar
with assembly file: /home/pkeni/git/hadoop-book/ch12/target/../../hive-examples.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch12 ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch12/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch12 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch12 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch12/target/../../hive-examples.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch12/3.0/ch12-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch12/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch12/3.0/ch12-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 13: HBase 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch13 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch13/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch13 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch13 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch13/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch13 ---
[INFO] Compiling 5 source files to /home/pkeni/git/hadoop-book/ch13/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch13 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch13/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch13 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch13 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch13 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch13/target/ch13-3.0.jar
[INFO] 
[INFO] --- maven-assembly-plugin:2.2.1:single (make-assembly) @ ch13 ---
[INFO] Reading assembly descriptor: ../book/src/main/assembly/jar.xml
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] HBaseTemperatureImporter$HBaseTemperatureMapper.class already added, skipping
[INFO] HBaseStationImporter.class already added, skipping
[INFO] HBaseTemperatureImporter.class already added, skipping
[INFO] HBaseTemperatureCli.class already added, skipping
[INFO] HBaseStationCli.class already added, skipping
[INFO] RowKeyConverter.class already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch13/target/../../hbase-examples.jar
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] HBaseTemperatureImporter$HBaseTemperatureMapper.class already added, skipping
[INFO] HBaseStationImporter.class already added, skipping
[INFO] HBaseTemperatureImporter.class already added, skipping
[INFO] HBaseTemperatureCli.class already added, skipping
[INFO] HBaseStationCli.class already added, skipping
[INFO] RowKeyConverter.class already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.
Instead of attaching the assembly file: /home/pkeni/git/hadoop-book/ch13/target/../../hbase-examples.jar, it will become the file for main project artifact.
NOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!
[WARNING] Replacing pre-existing project main-artifact file: /home/pkeni/git/hadoop-book/ch13/target/ch13-3.0.jar
with assembly file: /home/pkeni/git/hadoop-book/ch13/target/../../hbase-examples.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch13 ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch13/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch13 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch13 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch13/target/../../hbase-examples.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch13/3.0/ch13-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch13/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch13/3.0/ch13-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 14: ZooKeeper 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch14 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch14/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch14 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch14 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch14/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch14 ---
[INFO] Compiling 10 source files to /home/pkeni/git/hadoop-book/ch14/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch14 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch14/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch14 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch14 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch14 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch14/target/ch14-3.0.jar
[INFO] 
[INFO] --- maven-assembly-plugin:2.2.1:single (make-assembly) @ ch14 ---
[INFO] Reading assembly descriptor: ../book/src/main/assembly/jar.xml
[INFO] DeleteGroup.class already added, skipping
[INFO] CreateGroup.class already added, skipping
[INFO] ConfigUpdater.class already added, skipping
[INFO] ResilientConfigUpdater.class already added, skipping
[INFO] JoinGroup.class already added, skipping
[INFO] ListGroup.class already added, skipping
[INFO] ConnectionWatcher.class already added, skipping
[INFO] ConfigWatcher.class already added, skipping
[INFO] ResilientActiveKeyValueStore.class already added, skipping
[INFO] ActiveKeyValueStore.class already added, skipping
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch14/target/../../zookeeper-examples.jar
[INFO] DeleteGroup.class already added, skipping
[INFO] CreateGroup.class already added, skipping
[INFO] ConfigUpdater.class already added, skipping
[INFO] ResilientConfigUpdater.class already added, skipping
[INFO] JoinGroup.class already added, skipping
[INFO] ListGroup.class already added, skipping
[INFO] ConnectionWatcher.class already added, skipping
[INFO] ConfigWatcher.class already added, skipping
[INFO] ResilientActiveKeyValueStore.class already added, skipping
[INFO] ActiveKeyValueStore.class already added, skipping
[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.
Instead of attaching the assembly file: /home/pkeni/git/hadoop-book/ch14/target/../../zookeeper-examples.jar, it will become the file for main project artifact.
NOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!
[WARNING] Replacing pre-existing project main-artifact file: /home/pkeni/git/hadoop-book/ch14/target/ch14-3.0.jar
with assembly file: /home/pkeni/git/hadoop-book/ch14/target/../../zookeeper-examples.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch14 ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch14/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch14 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch14 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch14/target/../../zookeeper-examples.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch14/3.0/ch14-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch14/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch14/3.0/ch14-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 15: Sqoop 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch15 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch15/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch15 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch15 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch15/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch15 ---
[INFO] Compiling 3 source files to /home/pkeni/git/hadoop-book/ch15/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch15 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch15/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch15 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch15 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch15 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch15/target/ch15-3.0.jar
[INFO] 
[INFO] --- maven-assembly-plugin:2.2.1:single (make-assembly) @ ch15 ---
[INFO] Reading assembly descriptor: ../book/src/main/assembly/jar.xml
[INFO] MaxWidgetIdGenericAvro$MaxWidgetMapper.class already added, skipping
[INFO] Widget.class already added, skipping
[INFO] MaxWidgetId$MaxWidgetMapper.class already added, skipping
[INFO] MaxWidgetId$MaxWidgetReducer.class already added, skipping
[INFO] MaxWidgetId.class already added, skipping
[INFO] MaxWidgetIdGenericAvro.class already added, skipping
[INFO] MaxWidgetIdGenericAvro$MaxWidgetReducer.class already added, skipping
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch15/target/../../sqoop-examples.jar
[INFO] MaxWidgetIdGenericAvro$MaxWidgetMapper.class already added, skipping
[INFO] Widget.class already added, skipping
[INFO] MaxWidgetId$MaxWidgetMapper.class already added, skipping
[INFO] MaxWidgetId$MaxWidgetReducer.class already added, skipping
[INFO] MaxWidgetId.class already added, skipping
[INFO] MaxWidgetIdGenericAvro.class already added, skipping
[INFO] MaxWidgetIdGenericAvro$MaxWidgetReducer.class already added, skipping
[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.
Instead of attaching the assembly file: /home/pkeni/git/hadoop-book/ch15/target/../../sqoop-examples.jar, it will become the file for main project artifact.
NOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!
[WARNING] Replacing pre-existing project main-artifact file: /home/pkeni/git/hadoop-book/ch15/target/ch15-3.0.jar
with assembly file: /home/pkeni/git/hadoop-book/ch15/target/../../sqoop-examples.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch15 ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch15/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch15 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch15 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch15/target/../../sqoop-examples.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch15/3.0/ch15-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch15/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch15/3.0/ch15-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Chapter 16: Case Studies 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ ch16 ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/ch16/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ ch16 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ ch16 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch16/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ ch16 ---
[INFO] Compiling 2 source files to /home/pkeni/git/hadoop-book/ch16/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ ch16 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/ch16/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ ch16 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ ch16 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ ch16 ---
[INFO] Building jar: /home/pkeni/git/hadoop-book/ch16/target/ch16-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ ch16 ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/ch16/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ ch16 ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ ch16 ---
[INFO] Installing /home/pkeni/git/hadoop-book/ch16/target/ch16-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/ch16/3.0/ch16-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/ch16/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/ch16/3.0/ch16-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hadoop Examples JAR 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ hadoop-examples ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/hadoop-examples/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ hadoop-examples ---
[INFO] 
[INFO] --- maven-assembly-plugin:2.2.1:single (make-assembly) @ hadoop-examples ---
[INFO] Reading assembly descriptor: ../book/src/main/assembly/jar.xml
[WARNING] Cannot include project artifact: com.hadoopbook:hadoop-examples:pom:3.0; it doesn't have an associated file or directory.
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] Building jar: /home/pkeni/git/hadoop-book/hadoop-examples/target/../../hadoop-examples.jar
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] META-INF/ already added, skipping
[INFO] META-INF/MANIFEST.MF already added, skipping
[INFO] oldapi/ already added, skipping
[INFO] META-INF/maven/ already added, skipping
[INFO] META-INF/maven/com.hadoopbook/ already added, skipping
[INFO] 
[INFO] --- maven-assembly-plugin:2.2.1:single (oozie-workflow-application) @ hadoop-examples ---
[INFO] Reading assembly descriptor: ../book/src/main/assembly/oozie-workflow-application.xml
[INFO] Copying files to /home/pkeni/git/hadoop-book/hadoop-examples/target/.
[WARNING] Assembly file: /home/pkeni/git/hadoop-book/hadoop-examples/target/. is not a regular file (it may be a directory). It cannot be attached to the project build for installation or deployment.
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ hadoop-examples ---
[INFO] No tests to run.
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/hadoop-examples/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ hadoop-examples ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ hadoop-examples ---
[INFO] Installing /home/pkeni/git/hadoop-book/hadoop-examples/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/hadoop-examples/3.0/hadoop-examples-3.0.pom
[INFO] Installing /home/pkeni/git/hadoop-book/hadoop-examples/target/../../hadoop-examples.jar to /home/pkeni/.m2/repository/com/hadoopbook/hadoop-examples/3.0/hadoop-examples-3.0.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Snippet testing 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ snippet ---
[INFO] Deleting file set: /home/pkeni/git/hadoop-book/snippet/target (included: [**], excluded: [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0.1:enforce (enforce-versions) @ snippet ---
[INFO] 
[INFO] --- maven-resources-plugin:2.3:resources (default-resources) @ snippet ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/pkeni/git/hadoop-book/snippet/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ snippet ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.3:testResources (default-testResources) @ snippet ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ snippet ---
[INFO] Compiling 1 source file to /home/pkeni/git/hadoop-book/snippet/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.5:test (default-test) @ snippet ---
[INFO] Surefire report directory: /home/pkeni/git/hadoop-book/snippet/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
There are no tests to run.

Results :

Tests run: 0, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ snippet ---
[WARNING] JAR will be empty - no content was marked for inclusion!
[INFO] Building jar: /home/pkeni/git/hadoop-book/snippet/target/snippet-3.0.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:integration-test (integration-test) @ snippet ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/snippet/target/failsafe-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running ExamplesIT
0: /home/pkeni/git/hadoop-book/ch02/src/main/examples/OldMaxTemperature
1: /home/pkeni/git/hadoop-book/ch02/src/main/examples/max_temperature_py
2: /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperatureWithCombiner
3: /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperature
4: /home/pkeni/git/hadoop-book/ch04/src/main/examples/MaxTemperatureWithCompression
5: /home/pkeni/git/hadoop-book/ch04/src/main/examples/MaxTemperatureWithMapOutputCompression.ignore
6: /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3
7: /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV2GOP.ignore
8: /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3GOP
9: /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV2.ignore
10: /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationUsingMultipleOutputs
11: /home/pkeni/git/hadoop-book/ch07/src/main/examples/MaxTemperatureWithMultipleInputs
12: /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduceWithDefaults
13: /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationYearUsingMultipleOutputs
14: /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduce
15: /home/pkeni/git/hadoop-book/ch07/src/main/examples/SmallFilesToSequenceFileConverter.ignore
16: /home/pkeni/git/hadoop-book/ch08/src/main/examples/SortByTemperatureUsingHashPartitioner.ignore
17: /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureByStationNameUsingDistributedCacheFileApi.ignore
18: /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureUsingSecondarySort
19: /home/pkeni/git/hadoop-book/ch08/src/main/examples/SortDataPreprocessor.ignore
20: /home/pkeni/git/hadoop-book/ch08/src/main/examples/JoinRecordWithStationName
21: /home/pkeni/git/hadoop-book/ch08/src/main/examples/SortByTemperatureToMapFile.ignore
22: /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureWithCounters
mode=local
HADOOP_HOME=/home/pkeni/hadoop-1.1.1
version=1.1.1
Running /home/pkeni/git/hadoop-book/ch02/src/main/examples/OldMaxTemperature
Running input /home/pkeni/git/hadoop-book/ch02/src/main/examples/OldMaxTemperature/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:10:30 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:10:30 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/01/02 09:10:30 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:30 INFO mapred.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:31 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:10:31 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:10:31 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@40084706
14/01/02 09:10:31 INFO mapred.MapTask: numReduceTasks: 1
14/01/02 09:10:31 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:10:31 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:10:31 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:10:31 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:10:31 INFO mapred.MapTask: Finished spill 0
14/01/02 09:10:31 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:10:31 INFO mapred.LocalJobRunner: file:/home/pkeni/git/hadoop-book/input/ncdc/sample.txt:0+529
14/01/02 09:10:31 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:10:31 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@32c0bea2
14/01/02 09:10:31 INFO mapred.LocalJobRunner: 
14/01/02 09:10:31 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:10:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes
14/01/02 09:10:31 INFO mapred.LocalJobRunner: 
14/01/02 09:10:31 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:10:31 INFO mapred.LocalJobRunner: 
14/01/02 09:10:31 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:10:31 INFO mapred.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to file:/home/pkeni/git/hadoop-book/output
14/01/02 09:10:31 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:10:31 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:10:32 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:10:32 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:10:32 INFO mapred.JobClient: Counters: 21
14/01/02 09:10:32 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:10:32 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:10:32 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:10:32 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:10:32 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:10:32 INFO mapred.JobClient:     FILE_BYTES_READ=383625
14/01/02 09:10:32 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=454359
14/01/02 09:10:32 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:10:32 INFO mapred.JobClient:     Map output materialized bytes=61
14/01/02 09:10:32 INFO mapred.JobClient:     Map input records=5
14/01/02 09:10:32 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:10:32 INFO mapred.JobClient:     Spilled Records=10
14/01/02 09:10:32 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:10:32 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:10:32 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:10:32 INFO mapred.JobClient:     Map input bytes=529
14/01/02 09:10:32 INFO mapred.JobClient:     SPLIT_RAW_BYTES=107
14/01/02 09:10:32 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:10:32 INFO mapred.JobClient:     Reduce input records=5
14/01/02 09:10:32 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:10:32 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:10:32 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:10:32 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:10:32 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:10:32 INFO mapred.JobClient:     Map output records=5

Completed /home/pkeni/git/hadoop-book/ch02/src/main/examples/OldMaxTemperature
Running /home/pkeni/git/hadoop-book/ch02/src/main/examples/max_temperature_py
Running input /home/pkeni/git/hadoop-book/ch02/src/main/examples/max_temperature_py/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:10:35 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:10:35 WARN mapred.JobClient: No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).
14/01/02 09:10:35 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:35 INFO mapred.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:35 WARN mapred.LocalJobRunner: LocalJobRunner does not support symlinking into current working dir.
14/01/02 09:10:35 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-pkeni/mapred/local]
14/01/02 09:10:35 INFO streaming.StreamJob: Running job: job_local_0001
14/01/02 09:10:35 INFO streaming.StreamJob: Job running in-process (local Hadoop)
14/01/02 09:10:35 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:10:35 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@46306c48
14/01/02 09:10:35 INFO mapred.MapTask: numReduceTasks: 1
14/01/02 09:10:35 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:10:35 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:10:35 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:10:35 INFO streaming.PipeMapRed: PipeMapRed exec [/home/pkeni/git/hadoop-book/./ch02/src/main/python/max_temperature_map.py]
14/01/02 09:10:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]
14/01/02 09:10:35 INFO streaming.PipeMapRed: MRErrorThread done
14/01/02 09:10:35 INFO streaming.PipeMapRed: Records R/W=5/1
14/01/02 09:10:35 INFO streaming.PipeMapRed: mapRedFinished
14/01/02 09:10:35 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:10:35 INFO mapred.MapTask: Finished spill 0
14/01/02 09:10:35 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:10:35 INFO mapred.LocalJobRunner: Records R/W=5/1
14/01/02 09:10:35 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:10:35 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@518d3b19
14/01/02 09:10:35 INFO mapred.LocalJobRunner: 
14/01/02 09:10:35 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:10:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 67 bytes
14/01/02 09:10:35 INFO mapred.LocalJobRunner: 
14/01/02 09:10:35 INFO streaming.PipeMapRed: PipeMapRed exec [/home/pkeni/git/hadoop-book/./ch02/src/main/python/max_temperature_reduce.py]
14/01/02 09:10:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]
14/01/02 09:10:35 INFO streaming.PipeMapRed: Records R/W=5/1
14/01/02 09:10:35 INFO streaming.PipeMapRed: MRErrorThread done
14/01/02 09:10:35 INFO streaming.PipeMapRed: mapRedFinished
14/01/02 09:10:35 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:10:35 INFO mapred.LocalJobRunner: 
14/01/02 09:10:35 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:10:35 INFO mapred.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to file:/home/pkeni/git/hadoop-book/output
14/01/02 09:10:35 INFO mapred.LocalJobRunner: Records R/W=5/1 > reduce
14/01/02 09:10:35 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:10:36 INFO streaming.StreamJob:  map 100%  reduce 100%
14/01/02 09:10:36 INFO streaming.StreamJob: Job complete: job_local_0001
14/01/02 09:10:36 INFO streaming.StreamJob: Output: output

Completed /home/pkeni/git/hadoop-book/ch02/src/main/examples/max_temperature_py
Running /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperatureWithCombiner
Running input /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperatureWithCombiner/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:10:39 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:10:39 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/01/02 09:10:39 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:39 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:39 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:10:39 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:10:39 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@6cb4f39c
14/01/02 09:10:39 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:10:39 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:10:39 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:10:39 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:10:39 INFO mapred.MapTask: Finished spill 0
14/01/02 09:10:39 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:10:39 INFO mapred.LocalJobRunner: 
14/01/02 09:10:39 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:10:39 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@55cf055c
14/01/02 09:10:39 INFO mapred.LocalJobRunner: 
14/01/02 09:10:39 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:10:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24 bytes
14/01/02 09:10:39 INFO mapred.LocalJobRunner: 
14/01/02 09:10:39 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:10:39 INFO mapred.LocalJobRunner: 
14/01/02 09:10:39 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:10:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:10:39 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:10:39 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:10:40 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:10:40 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:10:40 INFO mapred.JobClient: Counters: 20
14/01/02 09:10:40 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:10:40 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:10:40 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:10:40 INFO mapred.JobClient:     FILE_BYTES_READ=383616
14/01/02 09:10:40 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=455005
14/01/02 09:10:40 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:10:40 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:10:40 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:10:40 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:10:40 INFO mapred.JobClient:     Map output materialized bytes=28
14/01/02 09:10:40 INFO mapred.JobClient:     Combine output records=2
14/01/02 09:10:40 INFO mapred.JobClient:     Map input records=5
14/01/02 09:10:40 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:10:40 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:10:40 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:10:40 INFO mapred.JobClient:     Spilled Records=4
14/01/02 09:10:40 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:10:40 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:10:40 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:10:40 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:10:40 INFO mapred.JobClient:     Combine input records=5
14/01/02 09:10:40 INFO mapred.JobClient:     Map output records=5
14/01/02 09:10:40 INFO mapred.JobClient:     SPLIT_RAW_BYTES=119
14/01/02 09:10:40 INFO mapred.JobClient:     Reduce input records=2

Completed /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperatureWithCombiner
Running /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperature
Running input /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperature/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:10:43 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:10:43 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/01/02 09:10:43 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:43 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:43 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:10:43 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:10:43 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@6cb4f39c
14/01/02 09:10:43 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:10:43 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:10:43 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:10:43 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:10:43 INFO mapred.MapTask: Finished spill 0
14/01/02 09:10:43 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:10:43 INFO mapred.LocalJobRunner: 
14/01/02 09:10:43 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:10:43 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@71fd03ab
14/01/02 09:10:43 INFO mapred.LocalJobRunner: 
14/01/02 09:10:43 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:10:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 57 bytes
14/01/02 09:10:43 INFO mapred.LocalJobRunner: 
14/01/02 09:10:43 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:10:43 INFO mapred.LocalJobRunner: 
14/01/02 09:10:43 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:10:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:10:43 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:10:43 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:10:44 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:10:44 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:10:44 INFO mapred.JobClient: Counters: 20
14/01/02 09:10:44 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:10:44 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:10:44 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:10:44 INFO mapred.JobClient:     FILE_BYTES_READ=383649
14/01/02 09:10:44 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=454687
14/01/02 09:10:44 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:10:44 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:10:44 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:10:44 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:10:44 INFO mapred.JobClient:     Map output materialized bytes=61
14/01/02 09:10:44 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:10:44 INFO mapred.JobClient:     Map input records=5
14/01/02 09:10:44 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:10:44 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:10:44 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:10:44 INFO mapred.JobClient:     Spilled Records=10
14/01/02 09:10:44 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:10:44 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:10:44 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:10:44 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:10:44 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:10:44 INFO mapred.JobClient:     Map output records=5
14/01/02 09:10:44 INFO mapred.JobClient:     SPLIT_RAW_BYTES=119
14/01/02 09:10:44 INFO mapred.JobClient:     Reduce input records=5

Completed /home/pkeni/git/hadoop-book/ch02/src/main/examples/MaxTemperature
Running /home/pkeni/git/hadoop-book/ch04/src/main/examples/MaxTemperatureWithCompression
Running input /home/pkeni/git/hadoop-book/ch04/src/main/examples/MaxTemperatureWithCompression/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:10:47 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:10:47 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/01/02 09:10:47 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:47 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:47 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:10:47 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:10:47 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@62469173
14/01/02 09:10:47 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:10:47 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:10:47 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:10:47 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
14/01/02 09:10:47 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:10:47 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:10:48 INFO mapred.MapTask: Finished spill 0
14/01/02 09:10:48 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:10:48 INFO mapred.LocalJobRunner: 
14/01/02 09:10:48 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:10:48 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@67385a81
14/01/02 09:10:48 INFO mapred.LocalJobRunner: 
14/01/02 09:10:48 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:10:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24 bytes
14/01/02 09:10:48 INFO mapred.LocalJobRunner: 
14/01/02 09:10:48 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:10:48 INFO mapred.LocalJobRunner: 
14/01/02 09:10:48 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:10:48 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:10:48 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:10:48 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:10:48 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:10:48 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:10:48 INFO mapred.JobClient: Counters: 20
14/01/02 09:10:48 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:10:48 INFO mapred.JobClient:     Bytes Written=49
14/01/02 09:10:48 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:10:48 INFO mapred.JobClient:     FILE_BYTES_READ=382898
14/01/02 09:10:48 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=455049
14/01/02 09:10:48 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:10:48 INFO mapred.JobClient:     Bytes Read=168
14/01/02 09:10:48 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:10:48 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:10:48 INFO mapred.JobClient:     Map output materialized bytes=28
14/01/02 09:10:48 INFO mapred.JobClient:     Combine output records=2
14/01/02 09:10:48 INFO mapred.JobClient:     Map input records=5
14/01/02 09:10:48 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:10:48 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:10:48 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:10:48 INFO mapred.JobClient:     Spilled Records=4
14/01/02 09:10:48 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:10:48 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:10:48 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:10:48 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:10:48 INFO mapred.JobClient:     Combine input records=5
14/01/02 09:10:48 INFO mapred.JobClient:     Map output records=5
14/01/02 09:10:48 INFO mapred.JobClient:     SPLIT_RAW_BYTES=122
14/01/02 09:10:48 INFO mapred.JobClient:     Reduce input records=2

Completed /home/pkeni/git/hadoop-book/ch04/src/main/examples/MaxTemperatureWithCompression
Running /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3
Running input /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:10:51 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:10:51 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:51 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:52 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:10:52 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:10:52 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@25830b71
14/01/02 09:10:52 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:10:52 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:10:52 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:10:52 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:10:52 INFO mapred.MapTask: Finished spill 0
14/01/02 09:10:52 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:10:52 INFO mapred.LocalJobRunner: 
14/01/02 09:10:52 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:10:52 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@43efe432
14/01/02 09:10:52 INFO mapred.LocalJobRunner: 
14/01/02 09:10:52 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:10:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24 bytes
14/01/02 09:10:52 INFO mapred.LocalJobRunner: 
14/01/02 09:10:52 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:10:52 INFO mapred.LocalJobRunner: 
14/01/02 09:10:52 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:10:52 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:10:52 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:10:52 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:10:53 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:10:53 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:10:53 INFO mapred.JobClient: Counters: 20
14/01/02 09:10:53 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:10:53 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:10:53 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:10:53 INFO mapred.JobClient:     FILE_BYTES_READ=383628
14/01/02 09:10:53 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=455377
14/01/02 09:10:53 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:10:53 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:10:53 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:10:53 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:10:53 INFO mapred.JobClient:     Map output materialized bytes=28
14/01/02 09:10:53 INFO mapred.JobClient:     Combine output records=2
14/01/02 09:10:53 INFO mapred.JobClient:     Map input records=5
14/01/02 09:10:53 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:10:53 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:10:53 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:10:53 INFO mapred.JobClient:     Spilled Records=4
14/01/02 09:10:53 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:10:53 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:10:53 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:10:53 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:10:53 INFO mapred.JobClient:     Combine input records=5
14/01/02 09:10:53 INFO mapred.JobClient:     Map output records=5
14/01/02 09:10:53 INFO mapred.JobClient:     SPLIT_RAW_BYTES=125
14/01/02 09:10:53 INFO mapred.JobClient:     Reduce input records=2

Completed /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3
Running /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3GOP
Running input /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3GOP/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:10:55 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:10:56 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:10:56 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:10:56 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:10:56 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:10:56 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@25830b71
14/01/02 09:10:56 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:10:56 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:10:56 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:10:56 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:10:56 INFO mapred.MapTask: Finished spill 0
14/01/02 09:10:56 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:10:56 INFO mapred.LocalJobRunner: 
14/01/02 09:10:56 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:10:56 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@43efe432
14/01/02 09:10:56 INFO mapred.LocalJobRunner: 
14/01/02 09:10:56 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:10:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24 bytes
14/01/02 09:10:56 INFO mapred.LocalJobRunner: 
14/01/02 09:10:56 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:10:56 INFO mapred.LocalJobRunner: 
14/01/02 09:10:56 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:10:56 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:10:56 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:10:56 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:10:57 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:10:57 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:10:57 INFO mapred.JobClient: Counters: 20
14/01/02 09:10:57 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:10:57 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:10:57 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:10:57 INFO mapred.JobClient:     FILE_BYTES_READ=383628
14/01/02 09:10:57 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=455385
14/01/02 09:10:57 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:10:57 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:10:57 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:10:57 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:10:57 INFO mapred.JobClient:     Map output materialized bytes=28
14/01/02 09:10:57 INFO mapred.JobClient:     Combine output records=2
14/01/02 09:10:57 INFO mapred.JobClient:     Map input records=5
14/01/02 09:10:57 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:10:57 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:10:57 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:10:57 INFO mapred.JobClient:     Spilled Records=4
14/01/02 09:10:57 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:10:57 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:10:57 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:10:57 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:10:57 INFO mapred.JobClient:     Combine input records=5
14/01/02 09:10:57 INFO mapred.JobClient:     Map output records=5
14/01/02 09:10:57 INFO mapred.JobClient:     SPLIT_RAW_BYTES=125
14/01/02 09:10:57 INFO mapred.JobClient:     Reduce input records=2

Completed /home/pkeni/git/hadoop-book/ch05/src/main/examples/MaxTemperatureDriverV3GOP
Running /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationUsingMultipleOutputs
Running input /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationUsingMultipleOutputs/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:00 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:00 INFO input.FileInputFormat: Total input paths to process : 2
14/01/02 09:11:00 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:00 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:00 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:00 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@1ef87e2e
14/01/02 09:11:00 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:00 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:00 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:00 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
14/01/02 09:11:00 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:11:00 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:00 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:00 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:00 INFO mapred.LocalJobRunner: 
14/01/02 09:11:00 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:00 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@eedc75d
14/01/02 09:11:00 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:00 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:00 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:01 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:01 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:01 INFO mapred.Task: Task:attempt_local_0001_m_000001_0 is done. And is in the process of commiting
14/01/02 09:11:01 INFO mapred.LocalJobRunner: 
14/01/02 09:11:01 INFO mapred.Task: Task 'attempt_local_0001_m_000001_0' done.
14/01/02 09:11:01 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@23930fb4
14/01/02 09:11:01 INFO mapred.LocalJobRunner: 
14/01/02 09:11:01 INFO mapred.Merger: Merging 2 sorted segments
14/01/02 09:11:01 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 2000308 bytes
14/01/02 09:11:01 INFO mapred.LocalJobRunner: 
14/01/02 09:11:01 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:01 INFO mapred.LocalJobRunner: 
14/01/02 09:11:01 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:01 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:01 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:01 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:01 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:01 INFO mapred.JobClient: Counters: 20
14/01/02 09:11:01 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:01 INFO mapred.JobClient:     Bytes Written=8
14/01/02 09:11:01 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:01 INFO mapred.JobClient:     FILE_BYTES_READ=2945092
14/01/02 09:11:01 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=7475603
14/01/02 09:11:01 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:01 INFO mapred.JobClient:     Bytes Read=147972
14/01/02 09:11:01 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:01 INFO mapred.JobClient:     Reduce input groups=6
14/01/02 09:11:01 INFO mapred.JobClient:     Map output materialized bytes=2000316
14/01/02 09:11:01 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:11:01 INFO mapred.JobClient:     Map input records=13130
14/01/02 09:11:01 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:01 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:01 INFO mapred.JobClient:     Reduce output records=0
14/01/02 09:11:01 INFO mapred.JobClient:     Spilled Records=26260
14/01/02 09:11:01 INFO mapred.JobClient:     Map output bytes=1960951
14/01/02 09:11:01 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:01 INFO mapred.JobClient:     Total committed heap usage (bytes)=942669824
14/01/02 09:11:01 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:01 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:11:01 INFO mapred.JobClient:     Map output records=13130
14/01/02 09:11:01 INFO mapred.JobClient:     SPLIT_RAW_BYTES=240
14/01/02 09:11:01 INFO mapred.JobClient:     Reduce input records=13130

Completed /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationUsingMultipleOutputs
Running /home/pkeni/git/hadoop-book/ch07/src/main/examples/MaxTemperatureWithMultipleInputs
Running input /home/pkeni/git/hadoop-book/ch07/src/main/examples/MaxTemperatureWithMultipleInputs/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:04 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:04 INFO input.FileInputFormat: Total input paths to process : 26
14/01/02 09:11:04 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:04 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:11:04 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:04 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:04 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@3da9b5c7
14/01/02 09:11:04 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:04 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:04 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@16a0b3a5
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000001_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000001_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@67f02555
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000002_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000002_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@5e184756
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000003_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000003_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@43f52b8
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000004_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000004_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@4e2f1185
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000005_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000005_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@15b64a3a
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000006_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000006_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@787086bd
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.JobClient:  map 100% reduce 0%
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000007_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000007_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@5bcbab86
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000008_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000008_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@4c95a8ce
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:05 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:05 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:05 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:05 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:05 INFO mapred.Task: Task:attempt_local_0001_m_000009_0 is done. And is in the process of commiting
14/01/02 09:11:05 INFO mapred.LocalJobRunner: 
14/01/02 09:11:05 INFO mapred.Task: Task 'attempt_local_0001_m_000009_0' done.
14/01/02 09:11:05 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@d344339
14/01/02 09:11:05 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000010_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000010_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@65a09e0b
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000011_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000011_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@4ed14172
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000012_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000012_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@76f9d876
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000013_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000013_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@72a3081f
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000014_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000014_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@6e984a73
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000015_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000015_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@24bbe429
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000016_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000016_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@57ae046b
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000017_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000017_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@7ae92232
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000018_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000018_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@35924daa
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000019_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000019_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@21f31079
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000020_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000020_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@1a84aa66
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000021_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000021_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@6dd2c810
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000022_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000022_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@546d1c90
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000023_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000023_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@e7f21d6
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000024_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000024_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@d694a33
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000025_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000025_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@231902c5
14/01/02 09:11:06 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:06 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:06 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:06 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:06 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_m_000026_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_m_000026_0' done.
14/01/02 09:11:06 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@5abc4ce7
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Merger: Merging 27 sorted segments
14/01/02 09:11:06 INFO mapred.Merger: Merging 9 intermediate segments out of a total of 27
14/01/02 09:11:06 INFO mapred.Merger: Merging 10 intermediate segments out of a total of 19
14/01/02 09:11:06 INFO mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 22130 bytes
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:06 INFO mapred.LocalJobRunner: 
14/01/02 09:11:06 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:06 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:06 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:06 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:06 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:06 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:06 INFO mapred.JobClient: Counters: 20
14/01/02 09:11:06 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:06 INFO mapred.JobClient:     Bytes Written=1424
14/01/02 09:11:06 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:06 INFO mapred.JobClient:     FILE_BYTES_READ=30807159
14/01/02 09:11:06 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=7081856
14/01/02 09:11:06 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:06 INFO mapred.JobClient:     Bytes Read=0
14/01/02 09:11:06 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:06 INFO mapred.JobClient:     Reduce input groups=156
14/01/02 09:11:06 INFO mapred.JobClient:     Map output materialized bytes=22272
14/01/02 09:11:06 INFO mapred.JobClient:     Combine output records=2010
14/01/02 09:11:06 INFO mapred.JobClient:     Map input records=24348
14/01/02 09:11:06 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:06 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:06 INFO mapred.JobClient:     Reduce output records=156
14/01/02 09:11:06 INFO mapred.JobClient:     Spilled Records=5013
14/01/02 09:11:06 INFO mapred.JobClient:     Map output bytes=214110
14/01/02 09:11:06 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:06 INFO mapred.JobClient:     Total committed heap usage (bytes)=17850433536
14/01/02 09:11:06 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:06 INFO mapred.JobClient:     Combine input records=23790
14/01/02 09:11:06 INFO mapred.JobClient:     Map output records=23790
14/01/02 09:11:06 INFO mapred.JobClient:     SPLIT_RAW_BYTES=8109
14/01/02 09:11:06 INFO mapred.JobClient:     Reduce input records=2010

Completed /home/pkeni/git/hadoop-book/ch07/src/main/examples/MaxTemperatureWithMultipleInputs
Running /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduceWithDefaults
Running input /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduceWithDefaults/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:09 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:09 INFO input.FileInputFormat: Total input paths to process : 2
14/01/02 09:11:09 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:10 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:10 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:10 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@27cacbd9
14/01/02 09:11:10 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:10 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:10 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:10 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
14/01/02 09:11:10 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:11:10 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:10 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:10 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:10 INFO mapred.LocalJobRunner: 
14/01/02 09:11:10 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:10 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@716859df
14/01/02 09:11:10 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:10 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:10 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:10 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:10 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:10 INFO mapred.Task: Task:attempt_local_0001_m_000001_0 is done. And is in the process of commiting
14/01/02 09:11:10 INFO mapred.LocalJobRunner: 
14/01/02 09:11:10 INFO mapred.Task: Task 'attempt_local_0001_m_000001_0' done.
14/01/02 09:11:10 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@32746450
14/01/02 09:11:10 INFO mapred.LocalJobRunner: 
14/01/02 09:11:10 INFO mapred.Merger: Merging 2 sorted segments
14/01/02 09:11:10 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 1934658 bytes
14/01/02 09:11:10 INFO mapred.LocalJobRunner: 
14/01/02 09:11:10 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:10 INFO mapred.LocalJobRunner: 
14/01/02 09:11:10 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:10 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:10 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:11 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:11 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:11 INFO mapred.JobClient: Counters: 20
14/01/02 09:11:11 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:11 INFO mapred.JobClient:     Bytes Written=1882034
14/01/02 09:11:11 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:11 INFO mapred.JobClient:     FILE_BYTES_READ=2879442
14/01/02 09:11:11 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=7405812
14/01/02 09:11:11 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:11 INFO mapred.JobClient:     Bytes Read=147972
14/01/02 09:11:11 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:11 INFO mapred.JobClient:     Reduce input groups=13124
14/01/02 09:11:11 INFO mapred.JobClient:     Map output materialized bytes=1934666
14/01/02 09:11:11 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:11:11 INFO mapred.JobClient:     Map input records=13130
14/01/02 09:11:11 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:11 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:11 INFO mapred.JobClient:     Reduce output records=13130
14/01/02 09:11:11 INFO mapred.JobClient:     Spilled Records=26260
14/01/02 09:11:11 INFO mapred.JobClient:     Map output bytes=1895301
14/01/02 09:11:11 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:11 INFO mapred.JobClient:     Total committed heap usage (bytes)=942669824
14/01/02 09:11:11 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:11 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:11:11 INFO mapred.JobClient:     Map output records=13130
14/01/02 09:11:11 INFO mapred.JobClient:     SPLIT_RAW_BYTES=240
14/01/02 09:11:11 INFO mapred.JobClient:     Reduce input records=13130

Completed /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduceWithDefaults
Running /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationYearUsingMultipleOutputs
Running input /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationYearUsingMultipleOutputs/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:13 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:13 INFO input.FileInputFormat: Total input paths to process : 2
14/01/02 09:11:13 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:14 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:14 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:14 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@3fb2acb7
14/01/02 09:11:14 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:14 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:14 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:14 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
14/01/02 09:11:14 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:11:14 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:14 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:14 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:14 INFO mapred.LocalJobRunner: 
14/01/02 09:11:14 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:14 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@66b875e1
14/01/02 09:11:14 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:14 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:14 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:14 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:14 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:14 INFO mapred.Task: Task:attempt_local_0001_m_000001_0 is done. And is in the process of commiting
14/01/02 09:11:14 INFO mapred.LocalJobRunner: 
14/01/02 09:11:14 INFO mapred.Task: Task 'attempt_local_0001_m_000001_0' done.
14/01/02 09:11:14 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@34267caa
14/01/02 09:11:14 INFO mapred.LocalJobRunner: 
14/01/02 09:11:14 INFO mapred.Merger: Merging 2 sorted segments
14/01/02 09:11:14 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 2000308 bytes
14/01/02 09:11:14 INFO mapred.LocalJobRunner: 
14/01/02 09:11:15 INFO mapred.JobClient:  map 100% reduce 0%
14/01/02 09:11:15 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:15 INFO mapred.LocalJobRunner: 
14/01/02 09:11:15 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:15 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:15 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:16 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:16 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:16 INFO mapred.JobClient: Counters: 20
14/01/02 09:11:16 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:16 INFO mapred.JobClient:     Bytes Written=8
14/01/02 09:11:16 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:16 INFO mapred.JobClient:     FILE_BYTES_READ=2945092
14/01/02 09:11:16 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=7475711
14/01/02 09:11:16 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:16 INFO mapred.JobClient:     Bytes Read=147972
14/01/02 09:11:16 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:16 INFO mapred.JobClient:     Reduce input groups=6
14/01/02 09:11:16 INFO mapred.JobClient:     Map output materialized bytes=2000316
14/01/02 09:11:16 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:11:16 INFO mapred.JobClient:     Map input records=13130
14/01/02 09:11:16 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:16 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:16 INFO mapred.JobClient:     Reduce output records=0
14/01/02 09:11:16 INFO mapred.JobClient:     Spilled Records=26260
14/01/02 09:11:16 INFO mapred.JobClient:     Map output bytes=1960951
14/01/02 09:11:16 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:16 INFO mapred.JobClient:     Total committed heap usage (bytes)=942669824
14/01/02 09:11:16 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:16 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:11:16 INFO mapred.JobClient:     Map output records=13130
14/01/02 09:11:16 INFO mapred.JobClient:     SPLIT_RAW_BYTES=240
14/01/02 09:11:16 INFO mapred.JobClient:     Reduce input records=13130

Completed /home/pkeni/git/hadoop-book/ch07/src/main/examples/PartitionByStationYearUsingMultipleOutputs
Running /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduce
Running input /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduce/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:19 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:19 INFO input.FileInputFormat: Total input paths to process : 2
14/01/02 09:11:19 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:19 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:19 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:19 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@63643de1
14/01/02 09:11:19 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:19 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:19 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:19 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
14/01/02 09:11:19 INFO compress.CodecPool: Got brand-new decompressor
14/01/02 09:11:19 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:19 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:19 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:19 INFO mapred.LocalJobRunner: 
14/01/02 09:11:19 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:19 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@29165a67
14/01/02 09:11:19 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:19 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:19 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:19 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:19 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:19 INFO mapred.Task: Task:attempt_local_0001_m_000001_0 is done. And is in the process of commiting
14/01/02 09:11:19 INFO mapred.LocalJobRunner: 
14/01/02 09:11:19 INFO mapred.Task: Task 'attempt_local_0001_m_000001_0' done.
14/01/02 09:11:19 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@31a4ef37
14/01/02 09:11:19 INFO mapred.LocalJobRunner: 
14/01/02 09:11:19 INFO mapred.Merger: Merging 2 sorted segments
14/01/02 09:11:19 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 1934658 bytes
14/01/02 09:11:19 INFO mapred.LocalJobRunner: 
14/01/02 09:11:20 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:20 INFO mapred.LocalJobRunner: 
14/01/02 09:11:20 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:20 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:20 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:20 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:20 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:20 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:20 INFO mapred.JobClient: Counters: 20
14/01/02 09:11:20 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:20 INFO mapred.JobClient:     Bytes Written=1882034
14/01/02 09:11:20 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:20 INFO mapred.JobClient:     FILE_BYTES_READ=2879442
14/01/02 09:11:20 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=7399602
14/01/02 09:11:20 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:20 INFO mapred.JobClient:     Bytes Read=147972
14/01/02 09:11:20 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:20 INFO mapred.JobClient:     Reduce input groups=13124
14/01/02 09:11:20 INFO mapred.JobClient:     Map output materialized bytes=1934666
14/01/02 09:11:20 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:11:20 INFO mapred.JobClient:     Map input records=13130
14/01/02 09:11:20 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:20 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:20 INFO mapred.JobClient:     Reduce output records=13130
14/01/02 09:11:20 INFO mapred.JobClient:     Spilled Records=26260
14/01/02 09:11:20 INFO mapred.JobClient:     Map output bytes=1895301
14/01/02 09:11:20 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:20 INFO mapred.JobClient:     Total committed heap usage (bytes)=942669824
14/01/02 09:11:20 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:20 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:11:20 INFO mapred.JobClient:     Map output records=13130
14/01/02 09:11:20 INFO mapred.JobClient:     SPLIT_RAW_BYTES=240
14/01/02 09:11:20 INFO mapred.JobClient:     Reduce input records=13130

Completed /home/pkeni/git/hadoop-book/ch07/src/main/examples/MinimalMapReduce
Running /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureUsingSecondarySort
Running input /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureUsingSecondarySort/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:23 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:23 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:11:23 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:23 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:23 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:23 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@5ca72da1
14/01/02 09:11:23 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:23 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:23 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:23 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:23 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:23 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:23 INFO mapred.LocalJobRunner: 
14/01/02 09:11:23 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:23 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@3e372b61
14/01/02 09:11:23 INFO mapred.LocalJobRunner: 
14/01/02 09:11:23 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:11:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 52 bytes
14/01/02 09:11:23 INFO mapred.LocalJobRunner: 
14/01/02 09:11:23 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:23 INFO mapred.LocalJobRunner: 
14/01/02 09:11:23 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:23 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:23 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:23 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:24 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:24 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:24 INFO mapred.JobClient: Counters: 20
14/01/02 09:11:24 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:24 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:11:24 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:24 INFO mapred.JobClient:     FILE_BYTES_READ=383644
14/01/02 09:11:24 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=456805
14/01/02 09:11:24 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:24 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:11:24 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:24 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:11:24 INFO mapred.JobClient:     Map output materialized bytes=56
14/01/02 09:11:24 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:11:24 INFO mapred.JobClient:     Map input records=5
14/01/02 09:11:24 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:24 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:24 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:11:24 INFO mapred.JobClient:     Spilled Records=10
14/01/02 09:11:24 INFO mapred.JobClient:     Map output bytes=40
14/01/02 09:11:24 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:24 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:11:24 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:24 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:11:24 INFO mapred.JobClient:     Map output records=5
14/01/02 09:11:24 INFO mapred.JobClient:     SPLIT_RAW_BYTES=119
14/01/02 09:11:24 INFO mapred.JobClient:     Reduce input records=5

Completed /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureUsingSecondarySort
Running /home/pkeni/git/hadoop-book/ch08/src/main/examples/JoinRecordWithStationName
Running input /home/pkeni/git/hadoop-book/ch08/src/main/examples/JoinRecordWithStationName/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:27 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:27 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:11:27 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:27 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:11:27 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:27 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:27 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@ea40029
14/01/02 09:11:27 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:27 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:27 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:28 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:28 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:28 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:28 INFO mapred.LocalJobRunner: 
14/01/02 09:11:28 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:28 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@4c2a0c2f
14/01/02 09:11:28 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:28 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:28 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:28 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:28 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:28 INFO mapred.Task: Task:attempt_local_0001_m_000001_0 is done. And is in the process of commiting
14/01/02 09:11:28 INFO mapred.LocalJobRunner: 
14/01/02 09:11:28 INFO mapred.Task: Task 'attempt_local_0001_m_000001_0' done.
14/01/02 09:11:28 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@3d23b070
14/01/02 09:11:28 INFO mapred.LocalJobRunner: 
14/01/02 09:11:28 INFO mapred.Merger: Merging 2 sorted segments
14/01/02 09:11:28 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 1324327 bytes
14/01/02 09:11:28 INFO mapred.LocalJobRunner: 
14/01/02 09:11:28 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:28 INFO mapred.LocalJobRunner: 
14/01/02 09:11:28 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:28 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:28 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:28 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:28 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:28 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:28 INFO mapred.JobClient: Counters: 20
14/01/02 09:11:28 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:28 INFO mapred.JobClient:     Bytes Written=761
14/01/02 09:11:28 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:28 INFO mapred.JobClient:     FILE_BYTES_READ=8660873
14/01/02 09:11:28 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=4660932
14/01/02 09:11:28 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:28 INFO mapred.JobClient:     Bytes Read=0
14/01/02 09:11:28 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:28 INFO mapred.JobClient:     Reduce input groups=28164
14/01/02 09:11:28 INFO mapred.JobClient:     Map output materialized bytes=1324335
14/01/02 09:11:28 INFO mapred.JobClient:     Combine output records=0
14/01/02 09:11:28 INFO mapred.JobClient:     Map input records=28169
14/01/02 09:11:28 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:28 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:28 INFO mapred.JobClient:     Reduce output records=5
14/01/02 09:11:28 INFO mapred.JobClient:     Spilled Records=56338
14/01/02 09:11:28 INFO mapred.JobClient:     Map output bytes=1267985
14/01/02 09:11:28 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:28 INFO mapred.JobClient:     Total committed heap usage (bytes)=942669824
14/01/02 09:11:28 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:28 INFO mapred.JobClient:     Combine input records=0
14/01/02 09:11:28 INFO mapred.JobClient:     Map output records=28169
14/01/02 09:11:28 INFO mapred.JobClient:     SPLIT_RAW_BYTES=514
14/01/02 09:11:28 INFO mapred.JobClient:     Reduce input records=28169

Completed /home/pkeni/git/hadoop-book/ch08/src/main/examples/JoinRecordWithStationName
Running /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureWithCounters
Running input /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureWithCounters/input.txt
Warning: $HADOOP_HOME is deprecated.

WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
14/01/02 09:11:31 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/01/02 09:11:31 INFO input.FileInputFormat: Total input paths to process : 1
14/01/02 09:11:31 WARN snappy.LoadSnappy: Snappy native library not loaded
14/01/02 09:11:31 INFO mapred.JobClient: Running job: job_local_0001
14/01/02 09:11:31 INFO util.ProcessTree: setsid exited with exit code 0
14/01/02 09:11:31 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@25830b71
14/01/02 09:11:32 INFO mapred.MapTask: io.sort.mb = 100
14/01/02 09:11:32 INFO mapred.MapTask: data buffer = 79691776/99614720
14/01/02 09:11:32 INFO mapred.MapTask: record buffer = 262144/327680
14/01/02 09:11:32 INFO mapred.MapTask: Starting flush of map output
14/01/02 09:11:32 INFO mapred.MapTask: Finished spill 0
14/01/02 09:11:32 INFO mapred.Task: Task:attempt_local_0001_m_000000_0 is done. And is in the process of commiting
14/01/02 09:11:32 INFO mapred.LocalJobRunner: 
14/01/02 09:11:32 INFO mapred.Task: Task 'attempt_local_0001_m_000000_0' done.
14/01/02 09:11:32 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@d9cbc24
14/01/02 09:11:32 INFO mapred.LocalJobRunner: 
14/01/02 09:11:32 INFO mapred.Merger: Merging 1 sorted segments
14/01/02 09:11:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 24 bytes
14/01/02 09:11:32 INFO mapred.LocalJobRunner: 
14/01/02 09:11:32 INFO mapred.Task: Task:attempt_local_0001_r_000000_0 is done. And is in the process of commiting
14/01/02 09:11:32 INFO mapred.LocalJobRunner: 
14/01/02 09:11:32 INFO mapred.Task: Task attempt_local_0001_r_000000_0 is allowed to commit now
14/01/02 09:11:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_r_000000_0' to output
14/01/02 09:11:32 INFO mapred.LocalJobRunner: reduce > reduce
14/01/02 09:11:32 INFO mapred.Task: Task 'attempt_local_0001_r_000000_0' done.
14/01/02 09:11:32 INFO mapred.JobClient:  map 100% reduce 100%
14/01/02 09:11:32 INFO mapred.JobClient: Job complete: job_local_0001
14/01/02 09:11:32 INFO mapred.JobClient: Counters: 21
14/01/02 09:11:32 INFO mapred.JobClient:   File Output Format Counters 
14/01/02 09:11:32 INFO mapred.JobClient:     Bytes Written=29
14/01/02 09:11:32 INFO mapred.JobClient:   FileSystemCounters
14/01/02 09:11:32 INFO mapred.JobClient:     FILE_BYTES_READ=383616
14/01/02 09:11:32 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=455513
14/01/02 09:11:32 INFO mapred.JobClient:   TemperatureQuality
14/01/02 09:11:32 INFO mapred.JobClient:     1=5
14/01/02 09:11:32 INFO mapred.JobClient:   File Input Format Counters 
14/01/02 09:11:32 INFO mapred.JobClient:     Bytes Read=529
14/01/02 09:11:32 INFO mapred.JobClient:   Map-Reduce Framework
14/01/02 09:11:32 INFO mapred.JobClient:     Reduce input groups=2
14/01/02 09:11:32 INFO mapred.JobClient:     Map output materialized bytes=28
14/01/02 09:11:32 INFO mapred.JobClient:     Combine output records=2
14/01/02 09:11:32 INFO mapred.JobClient:     Map input records=5
14/01/02 09:11:32 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/01/02 09:11:32 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/01/02 09:11:32 INFO mapred.JobClient:     Reduce output records=2
14/01/02 09:11:32 INFO mapred.JobClient:     Spilled Records=4
14/01/02 09:11:32 INFO mapred.JobClient:     Map output bytes=45
14/01/02 09:11:32 INFO mapred.JobClient:     CPU time spent (ms)=0
14/01/02 09:11:32 INFO mapred.JobClient:     Total committed heap usage (bytes)=494927872
14/01/02 09:11:32 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/01/02 09:11:32 INFO mapred.JobClient:     Combine input records=5
14/01/02 09:11:32 INFO mapred.JobClient:     Map output records=5
14/01/02 09:11:32 INFO mapred.JobClient:     SPLIT_RAW_BYTES=119
14/01/02 09:11:32 INFO mapred.JobClient:     Reduce input records=2

Completed /home/pkeni/git/hadoop-book/ch08/src/main/examples/MaxTemperatureWithCounters
Tests run: 23, Failures: 0, Errors: 0, Skipped: 8, Time elapsed: 65.133 sec

Results :

Tests run: 23, Failures: 0, Errors: 0, Skipped: 8

[INFO] 
[INFO] --- maven-failsafe-plugin:2.9:verify (verify) @ snippet ---
[INFO] Failsafe report directory: /home/pkeni/git/hadoop-book/snippet/target/failsafe-reports
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ snippet ---
[INFO] Installing /home/pkeni/git/hadoop-book/snippet/target/snippet-3.0.jar to /home/pkeni/.m2/repository/com/hadoopbook/snippet/3.0/snippet-3.0.jar
[INFO] Installing /home/pkeni/git/hadoop-book/snippet/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/snippet/3.0/snippet-3.0.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hadoop: The Definitive Guide, Example Code 3.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.3:clean (default-clean) @ root ---
[INFO] 
[INFO] --- maven-install-plugin:2.3:install (default-install) @ root ---
[INFO] Installing /home/pkeni/git/hadoop-book/pom.xml to /home/pkeni/.m2/repository/com/hadoopbook/root/3.0/root-3.0.pom
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hadoop: The Definitive Guide, Project ............. SUCCESS [1.302s]
[INFO] Common Code ....................................... SUCCESS [3.055s]
[INFO] Chapter 2: MapReduce .............................. SUCCESS [0.937s]
[INFO] Chapter 3: The Hadoop Distributed Filesystem ...... SUCCESS [26.502s]
[INFO] Chapter 4: Hadoop I/O ............................. SUCCESS [3.339s]
[INFO] Chapter 5: Developing a MapReduce Application ..... SUCCESS [41.576s]
[INFO] Chapter 7: MapReduce Types and Formats ............ SUCCESS [2.378s]
[INFO] Chapter 8: MapReduce Features ..................... SUCCESS [2.547s]
[INFO] Chapter 11: Pig ................................... SUCCESS [2.312s]
[INFO] Chapter 12: Hive .................................. SUCCESS [0.961s]
[INFO] Chapter 13: HBase ................................. SUCCESS [1.035s]
[INFO] Chapter 14: ZooKeeper ............................. SUCCESS [0.641s]
[INFO] Chapter 15: Sqoop ................................. SUCCESS [0.991s]
[INFO] Chapter 16: Case Studies .......................... SUCCESS [2.694s]
[INFO] Hadoop Examples JAR ............................... SUCCESS [0.530s]
[INFO] Snippet testing ................................... SUCCESS [1:05.952s]
[INFO] Hadoop: The Definitive Guide, Example Code ........ SUCCESS [0.004s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:37.291s
[INFO] Finished at: Thu Jan 02 09:11:32 PST 2014
[INFO] Final Memory: 43M/1705M
[INFO] ------------------------------------------------------------------------
[pkeni@aristeia:~/git/hadoop-book]$ 
